{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25af8b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Raw_Observation_001_Set1.tiff\n",
      "  Boxes: [[1808, 189, 1822, 203], [2486, 505, 2497, 516], [1928, 533, 1940, 545], [1585, 690, 1603, 695], [1061, 765, 1080, 784], [3696, 888, 3709, 901], [1977, 948, 1988, 958], [3577, 958, 3589, 970], [3514, 967, 3525, 978], [2888, 972, 2900, 984], [3457, 1083, 3467, 1094], [3448, 1147, 3460, 1160], [2352, 1195, 2362, 1206], [190, 1384, 205, 1399], [1052, 1421, 1146, 1448], [3291, 1430, 3303, 1441], [1241, 1436, 1253, 1448], [2469, 1653, 2481, 1665], [2177, 1959, 2187, 1970], [3227, 2460, 3240, 2473], [678, 2496, 690, 2508], [3472, 2595, 3481, 2605], [1569, 2643, 1583, 2657], [187, 2742, 201, 2756], [2519, 2773, 2539, 2794], [1372, 2902, 1385, 2915], [1809, 3279, 1820, 3290], [1586, 3366, 1601, 3381], [3777, 3399, 3796, 3418], [1898, 3494, 1908, 3504], [924, 3507, 940, 3523], [3163, 3741, 3179, 3758], [2415, 3778, 2430, 3793], [1910, 4152, 1924, 4166]]\n",
      "  Labels: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_003_Set1.tiff\n",
      "  Boxes: [[1809, 188, 1823, 202], [2487, 504, 2498, 515], [1929, 532, 1941, 544], [1668, 689, 1686, 694], [1062, 764, 1081, 783], [3697, 888, 3710, 901], [1978, 947, 1988, 958], [3579, 958, 3590, 970], [3516, 967, 3526, 978], [2889, 971, 2901, 983], [3458, 1083, 3469, 1093], [3448, 1147, 3461, 1159], [2353, 1194, 2363, 1205], [1387, 1377, 1485, 1405], [191, 1383, 206, 1397], [3292, 1429, 3304, 1441], [1242, 1435, 1254, 1447], [2470, 1653, 2482, 1664], [2177, 1959, 2187, 1969], [3227, 2459, 3240, 2472], [678, 2494, 690, 2507], [3472, 2595, 3481, 2605], [1568, 2642, 1583, 2656], [186, 2740, 201, 2755], [2519, 2773, 2539, 2793], [1371, 2901, 1385, 2914], [4072, 3196, 4082, 3206], [1808, 3278, 1820, 3289], [1585, 3364, 1601, 3380], [3776, 3399, 3796, 3418], [1898, 3493, 1908, 3503], [923, 3506, 940, 3522], [3162, 3741, 3179, 3757], [2415, 3777, 2429, 3792], [1908, 4150, 1924, 4166]]\n",
      "  Labels: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_004_Set1.tiff\n",
      "  Boxes: [[1810, 188, 1824, 202], [2487, 504, 2499, 516], [1929, 531, 1942, 544], [1709, 689, 1727, 694], [1063, 764, 1082, 782], [3697, 888, 3710, 901], [1978, 947, 1989, 957], [3579, 958, 3591, 970], [3378, 964, 3388, 973], [3516, 967, 3527, 978], [2890, 971, 2902, 983], [3459, 1083, 3469, 1094], [3449, 1147, 3462, 1160], [2353, 1194, 2364, 1205], [1559, 1355, 1659, 1383], [191, 1382, 206, 1397], [3293, 1429, 3304, 1441], [1243, 1434, 1255, 1446], [2471, 1653, 2483, 1664], [2178, 1958, 2188, 1969], [3227, 2460, 3240, 2473], [679, 2494, 690, 2506], [3472, 2595, 3482, 2605], [1569, 2642, 1583, 2656], [187, 2740, 201, 2754], [2520, 2773, 2540, 2793], [1372, 2901, 1386, 2914], [1809, 3278, 1821, 3289], [1586, 3365, 1602, 3380], [3778, 3399, 3797, 3418], [1899, 3493, 1909, 3503], [924, 3505, 941, 3522], [3163, 3741, 3180, 3757], [2416, 3777, 2430, 3792], [1910, 4151, 1924, 4165]]\n",
      "  Labels: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_005_Set1.tiff\n",
      "  Boxes: [[1811, 188, 1825, 202], [2488, 504, 2500, 516], [1930, 532, 1942, 544], [1750, 689, 1769, 694], [1064, 764, 1082, 783], [3698, 889, 3711, 902], [1979, 947, 1990, 958], [3580, 959, 3592, 970], [3378, 964, 3388, 974], [3517, 967, 3528, 978], [2890, 971, 2902, 984], [3459, 1083, 3470, 1094], [3449, 1147, 3462, 1160], [2354, 1195, 2364, 1205], [1734, 1333, 1835, 1361], [192, 1383, 207, 1397], [3293, 1430, 3305, 1441], [1243, 1435, 1255, 1447], [2471, 1653, 2483, 1665], [2179, 1959, 2189, 1969], [3228, 2460, 3241, 2473], [680, 2495, 691, 2506], [3473, 2595, 3483, 2605], [1570, 2642, 1584, 2656], [188, 2740, 202, 2754], [2520, 2773, 2541, 2793], [1373, 2902, 1386, 2914], [1809, 3278, 1821, 3289], [1587, 3365, 1602, 3380], [3777, 3400, 3796, 3419], [1899, 3493, 1909, 3503], [924, 3506, 941, 3522], [3163, 3741, 3180, 3758], [2416, 3777, 2431, 3792], [1910, 4151, 1924, 4165]]\n",
      "  Labels: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_006_Set1.tiff\n",
      "  Boxes: [[1812, 187, 1826, 201], [2489, 504, 2501, 515], [1931, 531, 1943, 543], [1791, 689, 1810, 694], [1065, 763, 1083, 781], [3699, 888, 3712, 901], [1980, 946, 1990, 957], [3581, 959, 3592, 970], [3518, 967, 3528, 978], [2891, 971, 2903, 983], [3460, 1083, 3470, 1093], [3450, 1147, 3463, 1159], [2355, 1194, 2365, 1204], [1912, 1312, 2014, 1339], [193, 1381, 207, 1396], [3294, 1429, 3305, 1441], [1244, 1434, 1256, 1446], [2472, 1652, 2484, 1664], [2179, 1958, 2189, 1968], [3228, 2460, 3241, 2472], [681, 2494, 691, 2504], [1570, 2641, 1584, 2655], [188, 2739, 202, 2753], [2520, 2772, 2541, 2793], [1373, 2900, 1386, 2913], [4074, 3197, 4083, 3207], [1810, 3277, 1821, 3288], [1586, 3363, 1602, 3379], [3778, 3399, 3797, 3418], [1899, 3493, 1909, 3502], [925, 3504, 941, 3521], [3163, 3741, 3180, 3757], [2416, 3777, 2430, 3792], [1910, 4150, 1924, 4164]]\n",
      "  Labels: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_007_Set1.tiff\n",
      "  Boxes: [[1814, 188, 1828, 202], [2491, 504, 2502, 516], [1933, 532, 1945, 544], [1833, 689, 1851, 694], [1066, 763, 1085, 782], [3701, 889, 3713, 902], [1982, 947, 1992, 957], [3583, 960, 3594, 971], [3520, 969, 3530, 979], [2893, 972, 2904, 984], [3462, 1085, 3472, 1094], [3452, 1148, 3464, 1160], [2357, 1195, 2366, 1205], [2093, 1290, 2196, 1317], [194, 1381, 209, 1396], [3296, 1431, 3307, 1442], [1246, 1434, 1257, 1446], [2474, 1653, 2485, 1665], [3230, 2461, 3242, 2473], [681, 2493, 692, 2505], [1571, 2642, 1585, 2656], [189, 2739, 203, 2753], [2522, 2773, 2542, 2794], [1374, 2901, 1387, 2914], [1810, 3278, 1822, 3289], [1588, 3364, 1603, 3379], [3779, 3400, 3798, 3420], [925, 3505, 942, 3521], [3164, 3742, 3180, 3758], [2416, 3778, 2431, 3792], [1910, 4151, 1924, 4165]]\n",
      "  Labels: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_008_Set1.tiff\n",
      "  Boxes: [[1814, 187, 1828, 201], [2492, 504, 2503, 516], [1933, 531, 1946, 543], [1874, 689, 1892, 694], [1067, 763, 1085, 781], [3701, 889, 3714, 902], [1982, 947, 1993, 957], [3583, 959, 3594, 971], [3520, 968, 3530, 979], [2893, 972, 2905, 984], [3462, 1084, 3473, 1094], [3453, 1148, 3465, 1160], [2357, 1194, 2367, 1205], [2276, 1268, 2381, 1295], [195, 1381, 209, 1395], [3296, 1430, 3307, 1441], [1246, 1434, 1258, 1445], [2474, 1653, 2485, 1664], [2181, 1958, 2191, 1968], [3230, 2460, 3243, 2473], [682, 2493, 693, 2504], [1571, 2641, 1586, 2655], [189, 2738, 204, 2752], [2522, 2773, 2542, 2793], [1374, 2900, 1387, 2913], [1811, 3277, 1822, 3288], [1588, 3364, 1603, 3379], [3779, 3400, 3798, 3419], [925, 3504, 942, 3520], [3164, 3742, 3180, 3758], [2416, 3777, 2431, 3792], [1910, 4150, 1924, 4164]]\n",
      "  Labels: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_009_Set1.tiff\n",
      "  Boxes: [[1816, 188, 1829, 202], [2492, 505, 2504, 516], [1934, 532, 1947, 544], [1915, 689, 1934, 695], [1068, 763, 1087, 782], [3702, 890, 3715, 903], [1983, 947, 1994, 958], [3584, 960, 3595, 972], [3521, 969, 3532, 980], [2894, 973, 2906, 984], [3463, 1085, 3474, 1095], [3453, 1149, 3466, 1161], [2357, 1195, 2368, 1206], [2462, 1246, 2568, 1274], [196, 1381, 210, 1395], [3297, 1431, 3308, 1443], [1247, 1434, 1258, 1446], [2475, 1654, 2486, 1665], [2182, 1959, 2191, 1969], [3230, 2461, 3243, 2474], [683, 2493, 693, 2504], [1572, 2642, 1586, 2656], [190, 2738, 204, 2752], [2522, 2774, 2542, 2794], [1375, 2901, 1388, 2913], [1811, 3278, 1822, 3289], [1588, 3364, 1603, 3379], [3779, 3402, 3798, 3420], [925, 3504, 942, 3521], [3164, 3743, 3181, 3759], [2416, 3778, 2431, 3793], [1909, 4150, 1925, 4166]]\n",
      "  Labels: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_010_Set1.tiff\n",
      "  Boxes: [[1816, 188, 1830, 202], [2493, 505, 2505, 516], [1935, 532, 1948, 544], [1954, 688, 1978, 697], [1069, 763, 1087, 782], [3703, 890, 3716, 903], [1984, 947, 1995, 958], [3585, 961, 3596, 972], [3522, 970, 3532, 980], [2895, 973, 2907, 984], [3464, 1086, 3474, 1095], [3454, 1149, 3467, 1161], [2358, 1195, 2369, 1206], [2649, 1225, 2757, 1252], [196, 1381, 211, 1395], [3298, 1432, 3309, 1443], [1247, 1434, 1259, 1446], [2475, 1654, 2487, 1665], [2182, 1959, 2192, 1969], [3231, 2462, 3244, 2474], [683, 2493, 694, 2504], [1573, 2642, 1587, 2656], [191, 2738, 205, 2752], [2523, 2774, 2543, 2794], [1376, 2901, 1389, 2914], [1812, 3278, 1823, 3289], [1588, 3364, 1604, 3379], [3780, 3402, 3799, 3421], [1901, 3494, 1911, 3503], [926, 3504, 943, 3521], [3165, 3743, 3181, 3759], [2417, 3778, 2432, 3793], [1911, 4151, 1925, 4165]]\n",
      "  Labels: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_011_Set1.tiff\n",
      "  Boxes: [[1818, 187, 1832, 201], [2495, 504, 2506, 516], [1937, 531, 1949, 543], [1995, 688, 2019, 697], [1070, 762, 1089, 780], [3704, 890, 3717, 902], [1985, 946, 1996, 957], [3586, 960, 3597, 972], [3523, 969, 3534, 979], [2896, 972, 2908, 984], [3465, 1084, 3476, 1095], [3455, 1148, 3468, 1161], [2359, 1194, 2370, 1205], [2839, 1203, 2948, 1231], [198, 1379, 212, 1394], [3298, 1431, 3310, 1442], [1249, 1433, 1260, 1445], [2476, 1653, 2488, 1665], [2183, 1958, 2193, 1969], [3232, 2461, 3245, 2474], [684, 2491, 695, 2503], [3476, 2597, 3486, 2607], [1573, 2641, 1587, 2655], [191, 2737, 206, 2751], [2523, 2773, 2544, 2793], [1376, 2900, 1389, 2912], [1812, 3277, 1823, 3289], [1589, 3363, 1604, 3379], [3780, 3401, 3799, 3420], [1901, 3493, 1911, 3503], [927, 3503, 943, 3519], [3165, 3742, 3182, 3759], [2418, 3777, 2432, 3792], [1911, 4150, 1926, 4165]]\n",
      "  Labels: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_012_Set2.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 602, 1052, 614], [1467, 894, 1490, 917], [3474, 1050, 3491, 1067], [2862, 1262, 2879, 1278], [2340, 1361, 2360, 1381], [1439, 1362, 1455, 1377], [2269, 1407, 2292, 1431], [1140, 1553, 1155, 1567], [963, 1559, 982, 1578], [3462, 1603, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3221, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2533, 1863], [654, 1884, 670, 1899], [4313, 1899, 4336, 1922], [444, 1983, 459, 1998], [1483, 2079, 1499, 2095], [1719, 2371, 1737, 2388], [1407, 2623, 1425, 2641], [4097, 2628, 4116, 2647], [2610, 2636, 2625, 2653], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3693, 2022, 3710], [3796, 3742, 3814, 3757], [3428, 3954, 3444, 3971], [3056, 4023, 3070, 4037], [1607, 4302, 1617, 4312]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_013_Set2.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1380], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1430], [1141, 1554, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [1407, 2623, 1425, 2641], [4098, 2629, 4115, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3693, 2021, 3710], [3789, 3743, 3805, 3756], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_014_Set2.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1380], [1439, 1362, 1455, 1377], [2269, 1407, 2292, 1431], [1141, 1554, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [1407, 2623, 1425, 2641], [4098, 2629, 4116, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3265, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3693, 2021, 3710], [3758, 3741, 3774, 3755], [3428, 3954, 3445, 3971], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_015_Set2.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 602, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1380], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1431], [1141, 1553, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3221, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [1407, 2623, 1425, 2641], [4098, 2629, 4116, 2647], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3694, 2021, 3710], [3719, 3738, 3736, 3753], [3428, 3954, 3445, 3971], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_016_Set2.tiff\n",
      "  Boxes: [[3502, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1381], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1430], [1141, 1553, 1155, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1267, 1656], [3206, 1691, 3220, 1706], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [1407, 2623, 1425, 2641], [4098, 2629, 4116, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3265, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3694, 2021, 3710], [3681, 3736, 3697, 3750], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_017_Set2.tiff\n",
      "  Boxes: [[3502, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1381], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1430], [1141, 1553, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1267, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1973, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [1407, 2623, 1425, 2641], [4098, 2629, 4116, 2646], [2610, 2637, 2625, 2653], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3694, 2021, 3710], [3642, 3733, 3659, 3748], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_018_Set2.tiff\n",
      "  Boxes: [[3502, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2360, 1380], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1430], [1141, 1554, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1736, 2388], [1407, 2623, 1425, 2641], [4098, 2629, 4115, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3694, 2021, 3709], [3604, 3732, 3620, 3745], [3428, 3954, 3444, 3971], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_019_Set2.tiff\n",
      "  Boxes: [[3502, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2879, 1278], [2340, 1361, 2359, 1380], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1431], [1141, 1553, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1634, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1736, 2388], [1407, 2623, 1425, 2641], [4098, 2629, 4116, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3693, 2021, 3710], [3565, 3729, 3581, 3743], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_020_Set2.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 602, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1380], [1439, 1362, 1455, 1377], [2269, 1407, 2292, 1430], [1141, 1554, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1736, 2388], [1407, 2623, 1425, 2641], [4098, 2629, 4116, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2341, 3417], [1445, 3590, 1465, 3610], [2005, 3694, 2021, 3710], [3526, 3726, 3543, 3741], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_021_Set2.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1380], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1430], [1140, 1553, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1620], [1245, 1633, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1719, 2371, 1737, 2388], [1407, 2623, 1425, 2641], [4098, 2629, 4115, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3693, 2021, 3710], [3487, 3724, 3505, 3739], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_022_Set2.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2360, 1381], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1430], [1141, 1554, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3221, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [1407, 2623, 1425, 2641], [4098, 2629, 4115, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3694, 2021, 3710], [3456, 3722, 3474, 3737], [3428, 3954, 3444, 3971], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_023_Set3.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 602, 1052, 614], [1467, 894, 1490, 917], [3474, 1050, 3491, 1067], [2862, 1262, 2878, 1278], [2340, 1361, 2360, 1381], [1439, 1362, 1455, 1377], [2268, 1407, 2292, 1431], [1141, 1553, 1155, 1568], [963, 1559, 982, 1579], [3462, 1604, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3221, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [655, 1884, 670, 1899], [4313, 1899, 4336, 1922], [444, 1983, 459, 1998], [1483, 2079, 1499, 2096], [1719, 2371, 1737, 2388], [2823, 2583, 2840, 2594], [1407, 2623, 1425, 2641], [4097, 2628, 4116, 2647], [2610, 2637, 2626, 2653], [1941, 3127, 1957, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3611], [2006, 3693, 2021, 3710], [3428, 3954, 3445, 3971], [3056, 4023, 3070, 4037], [1607, 4302, 1617, 4313]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_024_Set3.tiff\n",
      "  Boxes: [[3502, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1381], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1431], [1141, 1554, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1267, 1656], [3206, 1691, 3220, 1706], [1949, 1764, 1973, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1719, 2371, 1737, 2388], [2767, 2582, 2783, 2593], [1407, 2623, 1425, 2641], [4098, 2629, 4115, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3693, 2021, 3709], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_025_Set3.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 603, 1051, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1380], [1439, 1362, 1455, 1377], [2269, 1407, 2292, 1431], [1141, 1554, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [2711, 2582, 2728, 2591], [1407, 2623, 1425, 2641], [4098, 2629, 4116, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3416], [1445, 3590, 1465, 3610], [2005, 3694, 2021, 3710], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_026_Set3.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1380], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1430], [1141, 1553, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1267, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [655, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [2655, 2581, 2672, 2590], [1407, 2623, 1425, 2641], [4098, 2629, 4116, 2646], [2610, 2637, 2625, 2653], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2006, 3694, 2021, 3710], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_027_Set3.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1380], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1430], [1141, 1553, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1634, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [2599, 2579, 2616, 2589], [1407, 2623, 1425, 2641], [4098, 2629, 4115, 2646], [2610, 2637, 2625, 2653], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3265, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3694, 2021, 3709], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_028_Set3.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 602, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1380], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1430], [1141, 1554, 1155, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1719, 2371, 1737, 2388], [2544, 2578, 2560, 2588], [1407, 2623, 1425, 2641], [4098, 2629, 4115, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3265, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3694, 2021, 3709], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_029_Set3.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1381], [1439, 1362, 1455, 1377], [2269, 1407, 2292, 1431], [1140, 1554, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [2488, 2577, 2504, 2587], [1407, 2623, 1425, 2641], [4098, 2629, 4115, 2646], [2610, 2637, 2625, 2653], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3694, 2021, 3709], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_030_Set3.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2360, 1380], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1431], [1141, 1554, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1736, 2388], [2432, 2575, 2448, 2586], [1407, 2623, 1425, 2641], [4098, 2629, 4115, 2646], [2610, 2637, 2625, 2653], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3693, 2021, 3710], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_031_Set3.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2360, 1380], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1431], [1141, 1553, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1620], [1245, 1633, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [2377, 2574, 2393, 2585], [1407, 2623, 1425, 2641], [4098, 2629, 4116, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3265, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3693, 2021, 3710], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_032_Set3.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 602, 1051, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2359, 1380], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1430], [1140, 1554, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1620], [1245, 1634, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [2321, 2573, 2338, 2583], [1407, 2623, 1425, 2641], [4098, 2629, 4115, 2646], [2610, 2637, 2625, 2652], [1942, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3694, 2021, 3710], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Image: Raw_Observation_033_Set3.tiff\n",
      "  Boxes: [[3501, 566, 3514, 578], [1040, 603, 1052, 614], [1467, 894, 1490, 917], [3475, 1051, 3490, 1066], [2863, 1262, 2878, 1278], [2340, 1361, 2360, 1380], [1439, 1362, 1454, 1377], [2269, 1407, 2292, 1431], [1140, 1553, 1154, 1567], [963, 1559, 982, 1578], [3462, 1604, 3479, 1621], [1245, 1633, 1268, 1656], [3206, 1691, 3220, 1705], [1949, 1764, 1974, 1789], [2515, 1845, 2532, 1862], [656, 1885, 669, 1898], [4314, 1900, 4335, 1921], [445, 1984, 458, 1997], [1483, 2079, 1499, 2095], [1720, 2371, 1737, 2388], [2310, 2572, 2327, 2582], [1407, 2623, 1425, 2641], [4098, 2629, 4116, 2646], [2610, 2637, 2625, 2652], [1941, 3127, 1956, 3142], [1029, 3211, 1044, 3226], [1183, 3238, 1202, 3257], [357, 3264, 372, 3280], [2323, 3399, 2340, 3417], [1445, 3590, 1465, 3610], [2005, 3693, 2021, 3710], [3428, 3954, 3444, 3970], [3056, 4023, 3070, 4037]]\n",
      "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('eccentricity_data.csv')\n",
    "\n",
    "# Map object types to numeric labels\n",
    "label_map = {\"star\": 0, \"streak\": 1}\n",
    "\n",
    "# Group bounding boxes per image\n",
    "image_annotations = defaultdict(lambda: {\"boxes\": [], \"labels\": []})\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    image = row['image']\n",
    "    x1 = int(row['bbox_x'])\n",
    "    y1 = int(row['bbox_y'])\n",
    "    w = int(row['bbox_width'])\n",
    "    h = int(row['bbox_height'])\n",
    "    x2 = x1 + w\n",
    "    y2 = y1 + h\n",
    "    label = label_map[row['object_type']]\n",
    "\n",
    "    image_annotations[image][\"boxes\"].append([x1, y1, x2, y2])\n",
    "    image_annotations[image][\"labels\"].append(label)\n",
    "\n",
    "# Convert to lists for indexing\n",
    "image_paths = list(image_annotations.keys())\n",
    "annotations = [image_annotations[img] for img in image_paths]\n",
    "# Check if annotations are loaded correctly\n",
    "for img_name, annots in image_annotations.items():\n",
    "    print(f\"Image: {img_name}\")\n",
    "    print(f\"  Boxes: {annots['boxes']}\")  # Should be a list of [x1,y1,x2,y2]\n",
    "    print(f\"  Labels: {annots['labels']}\")  # Should be a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65a3bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import cv2  # For TIFF loading\n",
    "from torchvision.ops import box_convert\n",
    "class TIFFDetectionDataset(Dataset):\n",
    "    def __init__(self, image_paths, annotations, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.annotations = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Filter out empty annotations\n",
    "        for img_path, annot in zip(image_paths, annotations):\n",
    "            if len(annot[\"boxes\"]) > 0:\n",
    "                self.image_paths.append(img_path)\n",
    "                self.annotations.append(annot)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        annot = self.annotations[idx]\n",
    "        boxes = torch.tensor(annot[\"boxes\"], dtype=torch.float32)\n",
    "        labels = torch.tensor(annot[\"labels\"], dtype=torch.int64)\n",
    "\n",
    "        return img, {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3ba33fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1063, 8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2512593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in dataset: 0\n",
      "Error: Dataset is empty! Check paths/annotations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Load annotations (from your original code)\n",
    "df = pd.read_csv('eccentricity_data.csv')\n",
    "label_map = {\"star\": 0, \"streak\": 1}\n",
    "image_annotations = defaultdict(lambda: {\"boxes\": [], \"labels\": []})\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    image_name = row['image']\n",
    "    x1, y1, w, h = int(row['bbox_x']), int(row['bbox_y']), int(row['bbox_width']), int(row['bbox_height'])\n",
    "    image_annotations[image_name][\"boxes\"].append([x1, y1, x1 + w, y1 + h])\n",
    "    image_annotations[image_name][\"labels\"].append(label_map[row['object_type']])\n",
    "\n",
    "# Get all TIFF paths (assuming filenames in CSV match TIFFs in 'Raw_Images')\n",
    "image_paths = []\n",
    "for img_name in image_annotations.keys():\n",
    "    tiff_path = os.path.join(\"Datasets/Padded/\", f\"{img_name}\")  # Adjust extension if needed\n",
    "    if os.path.exists(tiff_path):\n",
    "        image_paths.append(tiff_path)\n",
    "\n",
    "# Split into train/val\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_paths, val_paths = train_test_split(image_paths, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets   \n",
    "train_dataset = TIFFDetectionDataset(train_paths, [image_annotations[os.path.splitext(os.path.basename(p))[0]] for p in train_paths])\n",
    "val_dataset = TIFFDetectionDataset(val_paths, [image_annotations[os.path.splitext(os.path.basename(p))[0]] for p in val_paths])\n",
    "print(f\"Total images in dataset: {len(train_dataset)}\")  # Should be > 0\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"Error: Dataset is empty! Check paths/annotations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f3c7811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "def get_fasterrcnn(num_classes):\n",
    "    # Load pre-trained Faster R-CNN with ResNet50-FPN backbone\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Replace the classifier head\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_fasterrcnn(num_classes=2)  # 2 classes: star (0), streak (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28a023bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# DataLoaders (handle varying image sizes)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_loader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTIFFDetectionDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m val_loader = DataLoader(\n\u001b[32m     10\u001b[39m     val_dataset, batch_size=\u001b[32m2\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     11\u001b[39m     collate_fn=TIFFDetectionDataset.collate_fn\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Optimizer and device\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:385\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    387\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:156\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoaders (handle varying image sizes)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=True,\n",
    "    collate_fn=TIFFDetectionDataset.collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=2, shuffle=False,\n",
    "    collate_fn=TIFFDetectionDataset.collate_fn\n",
    ")\n",
    "\n",
    "# Optimizer and device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{\n",
    "                    \"boxes\": t[\"boxes\"].to(device),\n",
    "                    \"labels\": t[\"labels\"].to(device)  # Required for classification\n",
    "                } for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {losses.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f9bf29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in dataset: 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total images in dataset: {len(dataset)}\")  # Should be > 0\n",
    "if len(dataset) == 0:\n",
    "    print(\"Error: Dataset is empty! Check paths/annotations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b60366d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your transformations (resize, normalization, etc)\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),  # Convert to PyTorch tensor\n",
    "    # Optional resizing\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TiffCv2WithEccentricityDataset(tiff_images, eccentricity_data, transform=transform)\n",
    "\n",
    "# Create a DataLoader\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f1bdf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "model = get_model(num_classes=3)  # 0: background, 1: star, 2: streak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fe89d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models.detection import FasterRCNN, FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Optional: to track progress in the loop\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61c37146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected target boxes to be a tensor of shape [N, 4], got torch.Size([0]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m eccentricities = [eccentricity.to(device) \u001b[38;5;28;01mfor\u001b[39;00m eccentricity \u001b[38;5;129;01min\u001b[39;00m eccentricities]  \u001b[38;5;66;03m# Eccentricities\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m loss_dict = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mboxes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# If you're including eccentricity in the loss calculation, you can add it here\u001b[39;00m\n\u001b[32m     21\u001b[39m  \u001b[38;5;66;03m# You will need to define this custom loss\u001b[39;00m\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Calculate total loss (sum of classification and regression losses)\u001b[39;00m\n\u001b[32m     24\u001b[39m losses = \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:67\u001b[39m, in \u001b[36mGeneralizedRCNN.forward\u001b[39m\u001b[34m(self, images, targets)\u001b[39m\n\u001b[32m     65\u001b[39m boxes = target[\u001b[33m\"\u001b[39m\u001b[33mboxes\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(boxes, torch.Tensor):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExpected target boxes to be a tensor of shape [N, 4], got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mboxes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     72\u001b[39m     torch._assert(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected target boxes to be of type Tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(boxes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\__init__.py:2150\u001b[39m, in \u001b[36m_assert\u001b[39m\u001b[34m(condition, message)\u001b[39m\n\u001b[32m   2144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.Tensor \u001b[38;5;129;01mand\u001b[39;00m overrides.has_torch_function(\n\u001b[32m   2145\u001b[39m     (condition,)\n\u001b[32m   2146\u001b[39m ):\n\u001b[32m   2147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m overrides.handle_torch_function(\n\u001b[32m   2148\u001b[39m         _assert, (condition,), condition, message\n\u001b[32m   2149\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2150\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
      "\u001b[31mAssertionError\u001b[39m: Expected target boxes to be a tensor of shape [N, 4], got torch.Size([0])."
     ]
    }
   ],
   "source": [
    "# Define the optimizer (Adam optimizer for simplicity)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "total_loss = 0\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for images, boxes, labels, eccentricities, filenames in tqdm(loader):  # Loop through the DataLoader\n",
    "        images = [img.to(device) for img in images]\n",
    "        boxes = [box.to(device) for box in boxes]  # Bounding boxes\n",
    "        labels = [label.to(device) for label in labels]  # Labels\n",
    "        eccentricities = [eccentricity.to(device) for eccentricity in eccentricities]  # Eccentricities\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets=[{'boxes': box, 'labels': label} for box, label in zip(boxes, labels)])\n",
    "        \n",
    "        # If you're including eccentricity in the loss calculation, you can add it here\n",
    "         # You will need to define this custom loss\n",
    "\n",
    "        # Calculate total loss (sum of classification and regression losses)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "        # Backpropagate and update weights\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}\")\n",
    "\n",
    "# Optionally save the model after training\n",
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf5d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c21a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('eccentricity_data.csv')\n",
    "\n",
    "# Group by image name since each image has multiple objects\n",
    "grouped = df.groupby('image')\n",
    "\n",
    "# Prepare the data structure expected by object detection frameworks\n",
    "data = []\n",
    "for image_name, group in grouped:\n",
    "    image_path = f\"Datasets/padded/{image_name}\"\n",
    "    boxes = group[['bbox_x', 'bbox_y', 'bbox_width', 'bbox_height']].values\n",
    "    labels = group['object_type'].apply(lambda x: 0 if x == 'star' else 1).values  # Convert to numeric labels\n",
    "    eccentricity = group['eccentricity'].values\n",
    "    \n",
    "    data.append({\n",
    "        'image_path': image_path,\n",
    "        'boxes': boxes,\n",
    "        'labels': labels,\n",
    "        'eccentricity': eccentricity\n",
    "    })\n",
    "\n",
    "# Split into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9491883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def convert_to_yolo_format(bbox, img_width, img_height):\n",
    "    # Convert from (x, y, width, height) to YOLO format (center_x, center_y, width, height) normalized\n",
    "    x_center = (bbox[0] + bbox[2] / 2) / img_width\n",
    "    y_center = (bbox[1] + bbox[3] / 2) / img_height\n",
    "    width = bbox[2] / img_width\n",
    "    height = bbox[3] / img_height\n",
    "    return [x_center, y_center, width, height]\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('yolo_dataset/images/train', exist_ok=True)\n",
    "os.makedirs('yolo_dataset/labels/train', exist_ok=True)\n",
    "os.makedirs('yolo_dataset/images/val', exist_ok=True)\n",
    "os.makedirs('yolo_dataset/labels/val', exist_ok=True)\n",
    "\n",
    "# Process each image\n",
    "for i, item in enumerate(train_data):\n",
    "    # Copy image to train directory\n",
    "    img = Image.open(item['image_path'])\n",
    "    img_width, img_height = img.size\n",
    "    img.save(f'yolo_dataset/images/train/{os.path.basename(item[\"image_path\"])}')\n",
    "    \n",
    "    # Create label file\n",
    "    label_path = f'yolo_dataset/labels/train/{os.path.basename(item[\"image_path\"]).split(\".\")[0]}.txt'\n",
    "    with open(label_path, 'w') as f:\n",
    "        for box, label in zip(item['boxes'], item['labels']):\n",
    "            yolo_box = convert_to_yolo_format(box, img_width, img_height)\n",
    "            f.write(f\"{label} {' '.join(map(str, yolo_box))}\\n\")\n",
    "\n",
    "# Repeat for test_data into val directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ec13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "20ac0581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.130  Python-3.13.3 torch-2.7.0+cu126 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset 'dataset.yaml' error  Dataset 'dataset.yaml' images not found, missing path 'C:\\projects\\Crystalgrowth\\datasets\\yolo_dataset\\images\\val'\nNote dataset download directory is 'C:\\projects\\Crystalgrowth\\datasets'. You can update this in 'C:\\Users\\thooy\\AppData\\Roaming\\Ultralytics\\settings.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:583\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.data.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33myaml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myml\u001b[39m\u001b[33m\"\u001b[39m} \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.task \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m    578\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdetect\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    579\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msegment\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    580\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    581\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mobb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    582\u001b[39m }:\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     data = \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\ultralytics\\data\\utils.py:454\u001b[39m, in \u001b[36mcheck_det_dataset\u001b[39m\u001b[34m(dataset, autodownload)\u001b[39m\n\u001b[32m    453\u001b[39m     m += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote dataset download directory is \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASETS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. You can update this in \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSETTINGS_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(m)\n\u001b[32m    455\u001b[39m t = time.time()\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Dataset 'dataset.yaml' images not found, missing path 'C:\\projects\\Crystalgrowth\\datasets\\yolo_dataset\\images\\val'\nNote dataset download directory is 'C:\\projects\\Crystalgrowth\\datasets'. You can update this in 'C:\\Users\\thooy\\AppData\\Roaming\\Ultralytics\\settings.json'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33myolov8n.pt\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# or yolov8s.pt, yolov8m.pt for larger models\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# use 'cpu' if no GPU available\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\ultralytics\\engine\\model.py:787\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    785\u001b[39m     args[\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.ckpt_path\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrainer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:139\u001b[39m, in \u001b[36mBaseTrainer.__init__\u001b[39m\u001b[34m(self, cfg, overrides, _callbacks)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28mself\u001b[39m.model = check_model_file_from_stem(\u001b[38;5;28mself\u001b[39m.args.model)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(LOCAL_RANK):  \u001b[38;5;66;03m# avoid auto-downloading dataset multiple times\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainset, \u001b[38;5;28mself\u001b[39m.testset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;28mself\u001b[39m.ema = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# Optimization utils init\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:587\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    585\u001b[39m             \u001b[38;5;28mself\u001b[39m.args.data = data[\u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(emojis(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_url(\u001b[38;5;28mself\u001b[39m.args.data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m error  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    588\u001b[39m \u001b[38;5;28mself\u001b[39m.data = data\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.single_cls:\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset 'dataset.yaml' error  Dataset 'dataset.yaml' images not found, missing path 'C:\\projects\\Crystalgrowth\\datasets\\yolo_dataset\\images\\val'\nNote dataset download directory is 'C:\\projects\\Crystalgrowth\\datasets'. You can update this in 'C:\\Users\\thooy\\AppData\\Roaming\\Ultralytics\\settings.json'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained model\n",
    "model = YOLO('yolov8n.pt')  # or yolov8s.pt, yolov8m.pt for larger models\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data='dataset.yaml',\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    device='0'  # use 'cpu' if no GPU available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "949ed871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Processing data into YOLO format...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=None and train_size=0.8, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 241\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(CONFIG[\u001b[33m'\u001b[39m\u001b[33moutput_dir\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m    240\u001b[39m     shutil.rmtree(CONFIG[\u001b[33m'\u001b[39m\u001b[33moutput_dir\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[43mprocess_data_to_yolo_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moutput_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Step 3: Train the model\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining YOLO model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mprocess_data_to_yolo_format\u001b[39m\u001b[34m(data, output_base_dir)\u001b[39m\n\u001b[32m    144\u001b[39m os.makedirs(os.path.join(output_base_dir, \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# Split data into train/val\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m train_data, val_data = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrandom_state\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Process training data\u001b[39;00m\n\u001b[32m    154\u001b[39m process_split(train_data, \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m, output_base_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2851\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2848\u001b[39m arrays = indexable(*arrays)\n\u001b[32m   2850\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2851\u001b[39m n_train, n_test = \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\n\u001b[32m   2853\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   2856\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2481\u001b[39m, in \u001b[36m_validate_shuffle_split\u001b[39m\u001b[34m(n_samples, test_size, train_size, default_test_size)\u001b[39m\n\u001b[32m   2478\u001b[39m n_train, n_test = \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[32m   2480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2481\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2482\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2483\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maforementioned parameters.\u001b[39m\u001b[33m\"\u001b[39m.format(n_samples, test_size, train_size)\n\u001b[32m   2485\u001b[39m     )\n\u001b[32m   2487\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[31mValueError\u001b[39m: With n_samples=0, test_size=None and train_size=0.8, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "import shutil\n",
    "\n",
    "# 1. Configuration\n",
    "CONFIG = {\n",
    "    'dataset_path': 'Datasets/padded',\n",
    "    'csv_path': 'eccentricity_data.csv',  # Update with your CSV path\n",
    "    'output_dir': 'yolo_dataset',\n",
    "    'tile_size': 1024,  # Optimal for 4500x4500 images\n",
    "    'tile_overlap': 0.2,\n",
    "    'model_type': 'yolov8l.pt',  # Larger model better for small objects\n",
    "    'train_size': 0.8,\n",
    "    'random_state': 42,\n",
    "    'img_extensions': ['.png', '.jpg', '.jpeg', '.tif']\n",
    "}\n",
    "\n",
    "# 2. Data Loading and Preparation\n",
    "def load_and_prepare_data(csv_path, dataset_path):\n",
    "    \"\"\"Load CSV and prepare data structure\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Filter only relevant image extensions\n",
    "    df['image'] = df['image'].apply(lambda x: x if os.path.splitext(x)[1].lower() in CONFIG['img_extensions'] else None)\n",
    "    df = df.dropna(subset=['image'])\n",
    "    \n",
    "    # Group by image\n",
    "    grouped = df.groupby('image')\n",
    "    \n",
    "    data = []\n",
    "    for image_name, group in grouped:\n",
    "        image_path = os.path.join(dataset_path, image_name)\n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "            \n",
    "        # Get image dimensions\n",
    "        with Image.open(image_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "        \n",
    "        # Convert boxes to absolute coordinates if they're normalized\n",
    "        boxes = group[['bbox_x', 'bbox_y', 'bbox_width', 'bbox_height']].values\n",
    "        if np.all(boxes[:, 2] <= 1) and np.all(boxes[:, 3] <= 1):  # If normalized\n",
    "            boxes[:, 0] *= img_width\n",
    "            boxes[:, 1] *= img_height\n",
    "            boxes[:, 2] *= img_width\n",
    "            boxes[:, 3] *= img_height\n",
    "        \n",
    "        # Convert to x1,y1,x2,y2 format\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]  # x2 = x + width\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]  # y2 = y + height\n",
    "        \n",
    "        labels = group['object_type'].apply(lambda x: 0 if x.lower() == 'star' else 1).values\n",
    "        \n",
    "        data.append({\n",
    "            'image_path': image_path,\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'original_size': (img_width, img_height)\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 3. Tiling Functions\n",
    "def tile_image(image_path, output_dir):\n",
    "    \"\"\"Split large image into smaller tiles with overlap\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    img_width, img_height = img.size\n",
    "    tile_size = CONFIG['tile_size']\n",
    "    tile_step = int(tile_size * (1 - CONFIG['tile_overlap']))\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    tiles = []\n",
    "    for y in range(0, img_height, tile_step):\n",
    "        for x in range(0, img_width, tile_step):\n",
    "            # Ensure we don't go past image boundaries\n",
    "            x_end = min(x + tile_size, img_width)\n",
    "            y_end = min(y + tile_size, img_height)\n",
    "            \n",
    "            # Skip if tile would be too small (adjust if needed)\n",
    "            if (x_end - x) < tile_size//2 or (y_end - y) < tile_size//2:\n",
    "                continue\n",
    "            \n",
    "            box = (x, y, x_end, y_end)\n",
    "            tile = img.crop(box)\n",
    "            \n",
    "            # Save tile\n",
    "            base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            tile_name = f\"{base_name}_{x}_{y}.png\"\n",
    "            tile_path = os.path.join(output_dir, tile_name)\n",
    "            tile.save(tile_path)\n",
    "            \n",
    "            tiles.append({\n",
    "                'tile_path': tile_path,\n",
    "                'original_coords': (x, y, x_end - x, y_end - y),\n",
    "                'original_image': image_path\n",
    "            })\n",
    "    return tiles\n",
    "\n",
    "def adjust_bbox_for_tile(original_bbox, tile_coords):\n",
    "    \"\"\"Convert original bbox coordinates to tile coordinates\"\"\"\n",
    "    tile_x, tile_y, tile_w, tile_h = tile_coords\n",
    "    x1, y1, x2, y2 = original_bbox\n",
    "    \n",
    "    # Check if bbox intersects with this tile\n",
    "    if (x2 < tile_x or x1 > tile_x + tile_w or\n",
    "        y2 < tile_y or y1 > tile_y + tile_h):\n",
    "        return None\n",
    "    \n",
    "    # Calculate intersection\n",
    "    new_x1 = max(0, x1 - tile_x)\n",
    "    new_y1 = max(0, y1 - tile_y)\n",
    "    new_x2 = min(tile_w, x2 - tile_x)\n",
    "    new_y2 = min(tile_h, y2 - tile_y)\n",
    "    \n",
    "    # Skip boxes that are too small after cropping\n",
    "    if (new_x2 - new_x1) < 5 or (new_y2 - new_y1) < 5:\n",
    "        return None\n",
    "    \n",
    "    return [new_x1, new_y1, new_x2, new_y2]\n",
    "\n",
    "# 4. YOLO Format Conversion\n",
    "def convert_to_yolo_format(bbox, tile_width, tile_height):\n",
    "    \"\"\"Convert from x1,y1,x2,y2 to YOLO format (normalized center x, center y, width, height)\"\"\"\n",
    "    width = bbox[2] - bbox[0]\n",
    "    height = bbox[3] - bbox[1]\n",
    "    x_center = (bbox[0] + bbox[2]) / 2 / tile_width\n",
    "    y_center = (bbox[1] + bbox[3]) / 2 / tile_height\n",
    "    width = width / tile_width\n",
    "    height = height / tile_height\n",
    "    return [x_center, y_center, width, height]\n",
    "\n",
    "# 5. Main Processing Function\n",
    "def process_data_to_yolo_format(data, output_base_dir):\n",
    "    \"\"\"Process all data into YOLO format with tiling\"\"\"\n",
    "    # Create directory structure\n",
    "    os.makedirs(os.path.join(output_base_dir, 'images', 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_base_dir, 'images', 'val'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_base_dir, 'labels', 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_base_dir, 'labels', 'val'), exist_ok=True)\n",
    "    \n",
    "    # Split data into train/val\n",
    "    train_data, val_data = train_test_split(\n",
    "        data, \n",
    "        train_size=CONFIG['train_size'], \n",
    "        random_state=CONFIG['random_state']\n",
    "    )\n",
    "    \n",
    "    # Process training data\n",
    "    process_split(train_data, 'train', output_base_dir)\n",
    "    \n",
    "    # Process validation data\n",
    "    process_split(val_data, 'val', output_base_dir)\n",
    "    \n",
    "    # Create dataset.yaml\n",
    "    create_yaml_file(output_base_dir)\n",
    "\n",
    "def process_split(data_split, split_name, output_base_dir):\n",
    "    \"\"\"Process a single split (train/val)\"\"\"\n",
    "    for item in data_split:\n",
    "        # Create tiles for the large image\n",
    "        tiles = tile_image(\n",
    "            item['image_path'],\n",
    "            os.path.join(output_base_dir, 'images', split_name)\n",
    "        )\n",
    "        \n",
    "        for tile in tiles:\n",
    "            # Get tile dimensions\n",
    "            with Image.open(tile['tile_path']) as img:\n",
    "                tile_width, tile_height = img.size\n",
    "            \n",
    "            # Adjust bounding boxes for this tile\n",
    "            adjusted_boxes = []\n",
    "            adjusted_labels = []\n",
    "            for box, label in zip(item['boxes'], item['labels']):\n",
    "                adjusted_box = adjust_bbox_for_tile(box, tile['original_coords'])\n",
    "                if adjusted_box:\n",
    "                    adjusted_boxes.append(adjusted_box)\n",
    "                    adjusted_labels.append(label)\n",
    "            \n",
    "            # Only proceed if there are objects in this tile\n",
    "            if adjusted_boxes:\n",
    "                # Create label file\n",
    "                label_file = os.path.splitext(os.path.basename(tile['tile_path']))[0] + '.txt'\n",
    "                label_path = os.path.join(output_base_dir, 'labels', split_name, label_file)\n",
    "                \n",
    "                with open(label_path, 'w') as f:\n",
    "                    for box, label in zip(adjusted_boxes, adjusted_labels):\n",
    "                        yolo_box = convert_to_yolo_format(box, tile_width, tile_height)\n",
    "                        f.write(f\"{label} {' '.join(map(str, yolo_box))}\\n\")\n",
    "\n",
    "def create_yaml_file(output_base_dir):\n",
    "    \"\"\"Create the dataset YAML file\"\"\"\n",
    "    content = f\"\"\"path: {os.path.abspath(output_base_dir)}\n",
    "train: images/train\n",
    "val: images/val\n",
    "\n",
    "names:\n",
    "  0: star\n",
    "  1: streak\n",
    "\"\"\"\n",
    "    with open(os.path.join(output_base_dir, 'dataset.yaml'), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "# 6. Training Function\n",
    "def train_yolo_model():\n",
    "    \"\"\"Train the YOLO model\"\"\"\n",
    "    # Load model\n",
    "    model = YOLO(CONFIG['model_type'])\n",
    "    \n",
    "    # Train\n",
    "    results = model.train(\n",
    "        data=os.path.join(CONFIG['output_dir'], 'dataset.yaml'),\n",
    "        epochs=100,\n",
    "        imgsz=CONFIG['tile_size'],\n",
    "        batch=8,  # Adjust based on GPU memory\n",
    "        device='0',  # Use GPU\n",
    "        patience=10,  # Early stopping patience\n",
    "        optimizer='AdamW',\n",
    "        lr0=0.001,\n",
    "        weight_decay=0.0005,\n",
    "        amp=True  # Mixed precision\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 7. Main Execution\n",
    "if __name__ == '__main__':\n",
    "    # Step 1: Load and prepare data\n",
    "    print(\"Loading and preparing data...\")\n",
    "    data = load_and_prepare_data(CONFIG['csv_path'], CONFIG['dataset_path'])\n",
    "    \n",
    "    # Step 2: Process into YOLO format with tiling\n",
    "    print(\"Processing data into YOLO format...\")\n",
    "    if os.path.exists(CONFIG['output_dir']):\n",
    "        shutil.rmtree(CONFIG['output_dir'])\n",
    "    process_data_to_yolo_format(data, CONFIG['output_dir'])\n",
    "    \n",
    "    # Step 3: Train the model\n",
    "    print(\"Training YOLO model...\")\n",
    "    train_yolo_model()\n",
    "    \n",
    "    print(\"Training complete! Model saved in runs/detect/train/\")\n",
    "\n",
    "# 8. Inference Function (for later use)\n",
    "def predict_large_image(model_path, large_image_path, output_dir=None, conf_thresh=0.25):\n",
    "    \"\"\"Run detection on large image using tiling approach\"\"\"\n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Create tiles\n",
    "    temp_tile_dir = 'temp_tiles'\n",
    "    tiles = tile_image(large_image_path, temp_tile_dir)\n",
    "    \n",
    "    all_results = []\n",
    "    for tile in tiles:\n",
    "        # Run detection\n",
    "        results = model(tile['tile_path'], conf=conf_thresh)\n",
    "        \n",
    "        # Convert detections back to original coordinates\n",
    "        tile_x, tile_y, tile_w, _ = tile['original_coords']\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "                conf = box.conf.item()\n",
    "                cls = box.cls.item()\n",
    "                \n",
    "                # Convert to original image coordinates\n",
    "                orig_x1 = x1 + tile_x\n",
    "                orig_y1 = y1 + tile_y\n",
    "                orig_x2 = x2 + tile_x\n",
    "                orig_y2 = y2 + tile_y\n",
    "                \n",
    "                all_results.append({\n",
    "                    'box': [orig_x1, orig_y1, orig_x2, orig_y2],\n",
    "                    'confidence': conf,\n",
    "                    'class': cls,\n",
    "                    'class_name': 'star' if cls == 0 else 'streak'\n",
    "                })\n",
    "    \n",
    "    # Clean up temporary tiles\n",
    "    shutil.rmtree(temp_tile_dir)\n",
    "    \n",
    "    # Visualize results if output directory specified\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with Image.open(large_image_path) as img:\n",
    "            for detection in all_results:\n",
    "                # Draw boxes (simplified)\n",
    "                # In practice, you'd use OpenCV or PIL drawing functions\n",
    "                pass\n",
    "            output_path = os.path.join(output_dir, os.path.basename(large_image_path))\n",
    "            img.save(output_path)\n",
    "    \n",
    "    return all_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

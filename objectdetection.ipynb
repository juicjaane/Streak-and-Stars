{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09a98571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "IMAGE_DIR = r\"Datasets\\Padded\"  # Directory where 4500x4500 images are stored\n",
    "CSV_PATH = \"eccentricity_data.csv\"  # CSV file with bounding boxes\n",
    "PATCH_SIZE = 1024  # Size of tiles to extract from the full image\n",
    "STRIDE = 1024      # Stride for sliding window tiling\n",
    "\n",
    "# Output directory to store tiles and processed annotations\n",
    "PATCH_OUTPUT_DIR = \"tiles/\"\n",
    "ANNOTATION_OUTPUT_PATH = \"tile_annotations.csv\"\n",
    "\n",
    "os.makedirs(PATCH_OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f376f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class StreakDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, transform=None, patch_size=256, stride=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            image_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied.\n",
    "            patch_size (int): Size of patches to extract from large image\n",
    "            stride (int): Stride for patch extraction\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Group annotations by image\n",
    "        self.image_groups = self.annotations.groupby('image')\n",
    "        \n",
    "        # Precompute all possible patches for all images\n",
    "        self.patches = []\n",
    "        for img_name, group in self.image_groups:\n",
    "            img_path = f\"{self.image_dir}/{img_name}\"\n",
    "            img_patches = self._extract_patches(img_path)\n",
    "            self.patches.extend(img_patches)\n",
    "    \n",
    "    def _extract_patches(self, img_path):\n",
    "        patches = []\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
    "        \n",
    "        if img is None:\n",
    "            raise ValueError(f\"Failed to load image at {img_path}\")\n",
    "        \n",
    "        height, width = img.shape  # OpenCV uses (height, width) format\n",
    "        \n",
    "        for y in range(0, height - self.patch_size + 1, self.stride):\n",
    "            for x in range(0, width - self.patch_size + 1, self.stride):\n",
    "                patch = img[y:y+self.patch_size, x:x+self.patch_size]\n",
    "                patch = Image.fromarray(patch)  # Convert back to PIL for compatibility\n",
    "                patches.append((patch, (x, y)))\n",
    "        \n",
    "        return patches\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patch, (x_offset, y_offset) = self.patches[idx]\n",
    "        img_name = self.image_groups.groups.keys()[0]  # Simplified\n",
    "        \n",
    "        if self.transform:\n",
    "            patch = self.transform(patch)\n",
    "        \n",
    "        # Get annotations for this patch\n",
    "        img_annotations = self.image_groups.get_group(img_name)\n",
    "        patch_annotations = []\n",
    "        \n",
    "        for _, row in img_annotations.iterrows():\n",
    "            # Check if annotation is within this patch\n",
    "            x, y, w, h = row['bbox_x'], row['bbox_y'], row['bbox_width'], row['bbox_height']\n",
    "            if (x_offset <= x < x_offset + self.patch_size and \n",
    "                y_offset <= y < y_offset + self.patch_size):\n",
    "                # Convert to patch coordinates\n",
    "                patch_x = x - x_offset\n",
    "                patch_y = y - y_offset\n",
    "                patch_annotations.append([patch_x, patch_y, w, h, 0 if row['object_type'] == 'star' else 1])\n",
    "        \n",
    "        # Convert to tensor\n",
    "        target = {\n",
    "            'boxes': torch.tensor([ann[:4] for ann in patch_annotations], dtype=torch.float32),\n",
    "            'labels': torch.tensor([ann[4] for ann in patch_annotations], dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        return patch, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5cc0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class StreakDetectionModel(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet50', pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load backbone\n",
    "        if backbone_name == 'resnet50':\n",
    "            backbone = torchvision.models.resnet50(pretrained=pretrained)\n",
    "            # Remove the fully connected layers\n",
    "            backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "            backbone.out_channels = 2048\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "        \n",
    "        # Custom anchor sizes and aspect ratios optimized for stars/streaks\n",
    "        # Stars are small and square, streaks are elongated\n",
    "        anchor_sizes = ((8,), (16,), (32,), (64,), (128,))\n",
    "        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "        \n",
    "        anchor_generator = AnchorGenerator(\n",
    "            sizes=anchor_sizes,\n",
    "            aspect_ratios=aspect_ratios\n",
    "        )\n",
    "        \n",
    "        # ROI pooling\n",
    "        roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "            featmap_names=['0'],\n",
    "            output_size=7,\n",
    "            sampling_ratio=2\n",
    "        )\n",
    "        \n",
    "        # Box head\n",
    "        box_head = self._create_box_head(backbone.out_channels)\n",
    "        \n",
    "        # Faster R-CNN model\n",
    "        self.model = FasterRCNN(\n",
    "            backbone,\n",
    "            num_classes=3,  # background, star, streak\n",
    "            rpn_anchor_generator=anchor_generator,\n",
    "            box_roi_pool=roi_pooler,\n",
    "            box_head=box_head,\n",
    "            box_predictor=None,  # Will be created internally\n",
    "            transform=CustomTransform()  # See below\n",
    "        )\n",
    "        \n",
    "        # Add directional prediction for streaks\n",
    "        self.direction_predictor = nn.Sequential(\n",
    "            nn.Linear(backbone.out_channels * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)  # Predicts angle in radians\n",
    "        )\n",
    "    \n",
    "    def _create_box_head(self, in_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_channels * 7 * 7, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    \n",
    "    def forward(self, images, targets=None):\n",
    "        if self.training and targets is None:\n",
    "            raise ValueError(\"In training mode, targets should be passed\")\n",
    "        \n",
    "        # Standard Faster R-CNN forward pass\n",
    "        outputs = self.model(images, targets)\n",
    "        \n",
    "        if not self.training:\n",
    "            # Add direction prediction for streaks in inference mode\n",
    "            for output in outputs:\n",
    "                if 'boxes' in output:\n",
    "                    # Get ROI features\n",
    "                    features = self.model.roi_heads.box_roi_pool(\n",
    "                        [self.model.backbone(img.unsqueeze(0))['0'] for img in images],\n",
    "                        [output['boxes']],\n",
    "                        [img.shape[-2:] for img in images]\n",
    "                    )\n",
    "                    features = features.flatten(1)\n",
    "                    \n",
    "                    # Predict direction\n",
    "                    directions = self.direction_predictor(features)\n",
    "                    output['directions'] = directions\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "class CustomTransform(GeneralizedRCNNTransform):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            min_size=512,  # Minimum size of the image\n",
    "            max_size=512,   # Maximum size of the image\n",
    "            image_mean=[0.5],  # Grayscale mean\n",
    "            image_std=[0.5]    # Grayscale std\n",
    "        )\n",
    "    \n",
    "    def postprocess(self, result, image_shapes, original_image_sizes):\n",
    "        # Override to handle our custom outputs\n",
    "        if isinstance(result, dict) and 'directions' in result:\n",
    "            # Convert boxes to original image space\n",
    "            boxes = result['boxes']\n",
    "            directions = result['directions']\n",
    "            \n",
    "            # Scale boxes back to original image size\n",
    "            # (implementation depends on your exact needs)\n",
    "            \n",
    "            return {\n",
    "                'boxes': boxes,\n",
    "                'labels': result['labels'],\n",
    "                'scores': result['scores'],\n",
    "                'directions': directions\n",
    "            }\n",
    "        return super().postprocess(result, image_shapes, original_image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33a41a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def get_transform(train):\n",
    "    transform_list = [transforms.ToTensor()]  # Start with converting to tensor\n",
    "    \n",
    "    if train:\n",
    "        # Add data augmentation only for training\n",
    "        transform_list.extend([\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomVerticalFlip(0.5),\n",
    "            transforms.RandomRotation(10)\n",
    "        ])\n",
    "    \n",
    "    # Combine all transforms\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "def train_model():\n",
    "    # Initialize dataset and dataloader\n",
    "    dataset = StreakDataset(\n",
    "        csv_file=CSV_PATH,\n",
    "        image_dir=IMAGE_DIR,\n",
    "        transform=get_transform(train=True),\n",
    "        patch_size=256,\n",
    "        stride=128\n",
    "    )\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = StreakDetectionModel(backbone_name='resnet50', pretrained=True)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer and learning rate scheduler\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    \n",
    "    # Loss function (included in Faster R-CNN)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, targets in data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Evaluation (implement as needed)\n",
    "        # evaluate(model, validation_loader, device)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'streak_detection_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f99d86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_streaks_full_image(model, image_path, patch_size=512, stride=256, device='cuda'):\n",
    "    # Load image\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    width, height = img.size\n",
    "    \n",
    "    # Initialize results\n",
    "    all_boxes = []\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    all_directions = []\n",
    "    \n",
    "    # Process image in patches\n",
    "    for y in range(0, height - patch_size + 1, stride):\n",
    "        for x in range(0, width - patch_size + 1, stride):\n",
    "            patch = img.crop((x, y, x + patch_size, y + patch_size))\n",
    "            patch_tensor = transforms.ToTensor()(patch).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                output = model(patch_tensor)[0]\n",
    "            \n",
    "            # Convert boxes to original image coordinates\n",
    "            boxes = output['boxes'].cpu().numpy()\n",
    "            boxes[:, [0, 2]] += x\n",
    "            boxes[:, [1, 3]] += y\n",
    "            \n",
    "            all_boxes.extend(boxes)\n",
    "            all_labels.extend(output['labels'].cpu().numpy())\n",
    "            all_scores.extend(output['scores'].cpu().numpy())\n",
    "            \n",
    "            if 'directions' in output:\n",
    "                all_directions.extend(output['directions'].cpu().numpy())\n",
    "    \n",
    "    # Apply non-maximum suppression to remove overlapping boxes\n",
    "    if len(all_boxes) > 0:\n",
    "        keep_indices = torchvision.ops.nms(\n",
    "            torch.tensor(all_boxes),\n",
    "            torch.tensor(all_scores),\n",
    "            iou_threshold=0.5\n",
    "        )\n",
    "        \n",
    "        final_boxes = [all_boxes[i] for i in keep_indices]\n",
    "        final_labels = [all_labels[i] for i in keep_indices]\n",
    "        final_scores = [all_scores[i] for i in keep_indices]\n",
    "        final_directions = [all_directions[i] for i in keep_indices] if all_directions else None\n",
    "        \n",
    "        return {\n",
    "            'boxes': final_boxes,\n",
    "            'labels': final_labels,\n",
    "            'scores': final_scores,\n",
    "            'directions': final_directions\n",
    "        }\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "402725b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model\u001b[39m():\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Initialize dataset and dataloader\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     dataset = \u001b[43mStreakDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMAGE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     data_loader = DataLoader(\n\u001b[32m     29\u001b[39m         dataset,\n\u001b[32m     30\u001b[39m         batch_size=\u001b[32m4\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m         collate_fn=collate_fn\n\u001b[32m     34\u001b[39m     )\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mStreakDataset.__init__\u001b[39m\u001b[34m(self, csv_file, image_dir, transform, patch_size, stride)\u001b[39m\n\u001b[32m     31\u001b[39m img = cv2.imread(img_path,cv2.IMREAD_UNCHANGED) \u001b[38;5;66;03m# Convert to grayscale\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Get all patches for this image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m img_patches = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.patches.extend(img_patches)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mStreakDataset._extract_patches\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_extract_patches\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     38\u001b[39m     patches = []\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     width, height = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, height - \u001b[38;5;28mself\u001b[39m.patch_size + \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.stride):\n\u001b[32m     42\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, width - \u001b[38;5;28mself\u001b[39m.patch_size + \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.stride):\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1277ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38a4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4744647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3193ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_image_and_annotations(image_path, annotations_df, patch_size=1024, stride=1024):\n",
    "    image_name = os.path.basename(image_path)\n",
    "    img = cv2.imread(image_path)\n",
    "    h, w, _ = img.shape\n",
    "    \n",
    "    image_annotations = annotations_df[annotations_df['image'] == image_name]\n",
    "    saved_tiles = []\n",
    "    \n",
    "    tile_id = 0\n",
    "    new_annotations = []\n",
    "\n",
    "    for y in range(0, h, stride):\n",
    "        for x in range(0, w, stride):\n",
    "            tile = img[y:y+patch_size, x:x+patch_size]\n",
    "            if tile.shape[0] < patch_size or tile.shape[1] < patch_size:\n",
    "                continue  # skip edge tiles\n",
    "\n",
    "            tile_boxes = []\n",
    "            for _, row in image_annotations.iterrows():\n",
    "                xmin, ymin = row['bbox_x'], row['bbox_y']\n",
    "                xmax = xmin + row['bbox_width']\n",
    "                ymax = ymin + row['bbox_height']\n",
    "\n",
    "                # Check if bbox intersects the tile\n",
    "                if xmax < x or xmin > x+patch_size or ymax < y or ymin > y+patch_size:\n",
    "                    continue  # no intersection\n",
    "\n",
    "                # Convert to local coordinates\n",
    "                new_xmin = max(xmin - x, 0)\n",
    "                new_ymin = max(ymin - y, 0)\n",
    "                new_xmax = min(xmax - x, patch_size)\n",
    "                new_ymax = min(ymax - y, patch_size)\n",
    "\n",
    "                box_width = new_xmax - new_xmin\n",
    "                box_height = new_ymax - new_ymin\n",
    "                if box_width > 0 and box_height > 0:\n",
    "                    tile_boxes.append({\n",
    "                        \"tile_name\": f\"{image_name}_tile_{tile_id}.png\",\n",
    "                        \"bbox_x\": new_xmin,\n",
    "                        \"bbox_y\": new_ymin,\n",
    "                        \"bbox_width\": box_width,\n",
    "                        \"bbox_height\": box_height,\n",
    "                        \"object_type\": row[\"object_type\"]\n",
    "                    })\n",
    "\n",
    "            if tile_boxes:\n",
    "                tile_filename = f\"{image_name}_tile_{tile_id}.png\"\n",
    "                tile_path = os.path.join(PATCH_OUTPUT_DIR, tile_filename)\n",
    "                cv2.imwrite(tile_path, tile)\n",
    "                saved_tiles.append(tile_filename)\n",
    "                new_annotations.extend(tile_boxes)\n",
    "                tile_id += 1\n",
    "\n",
    "    return new_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a56c2211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiling Raw_Observation_009_Set1.tiff...\n",
      "Saved 31 annotations for tiled patches.\n"
     ]
    }
   ],
   "source": [
    "# Load annotations\n",
    "annotations_df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# For all images in the directory\n",
    "all_new_annotations = []\n",
    "for image_file in os.listdir(IMAGE_DIR):\n",
    "    if image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "        image_path = os.path.join(IMAGE_DIR, image_file)\n",
    "        print(f\"Tiling {image_file}...\")\n",
    "        new_anns = tile_image_and_annotations(image_path, annotations_df)\n",
    "        all_new_annotations.extend(new_anns)\n",
    "\n",
    "# Save new tile-level annotations\n",
    "tile_df = pd.DataFrame(all_new_annotations)\n",
    "tile_df.to_csv(ANNOTATION_OUTPUT_PATH, index=False)\n",
    "print(f\"Saved {len(tile_df)} annotations for tiled patches.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f93dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

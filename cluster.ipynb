{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "593a48c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your image directory path\n",
    "image_dir = Path(r\"Datasets\\Raw_Images\")  # <-- change this to your actual folder path\n",
    "image_paths = list(image_dir.glob(\"*.tiff\"))\n",
    "output_dir = Path(r\"Processed\")  # <-- change this to your actual folder path\n",
    "# Output dictionary for processed binary images\n",
    "preprocessed_images = {}\n",
    "images_8bit = {}\n",
    "pre_thres={}\n",
    "# CLAHE setup\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "\n",
    "for path in image_paths:\n",
    "    # Load 16-bit grayscale image\n",
    "    img_16 = cv2.imread(str(path), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    if img_16 is None:\n",
    "        print(f\"Failed to load {path.name}\")\n",
    "        continue\n",
    "\n",
    "    # Step 1: Normalize to 8-bit\n",
    "    img_norm = cv2.normalize(img_16, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    img_8bit =  img_norm.astype(np.uint8)\n",
    "    images_8bit[path.name] = img_8bit\n",
    "    # Step 2: Gaussian Blur\n",
    "    bilateral_filtered = cv2.bilateralFilter(img_8bit, 9, 75, 75)  # Adjust parameters as needed\n",
    "\n",
    "    # Step 3: CLAHE\n",
    "    clahe_img = clahe.apply(bilateral_filtered)\n",
    "    pre_thres[path.name] = clahe_img\n",
    "    hist = cv2.calcHist([clahe_img], [0], None, [256], [0, 256])\n",
    "    hist = hist / hist.sum()  # Normalize to sum to 1 (optional)\n",
    "\n",
    "# Step 3: Calculate the Cumulative Histogram\n",
    "    cumulative_hist = np.cumsum(hist)\n",
    "\n",
    "    # Step 4: Find the bin corresponding to the median\n",
    "    median_bin = np.searchsorted(cumulative_hist, 0.5)\n",
    "\n",
    "    \n",
    "    # Step 4: Binary Thresholding\n",
    "    _, binary = cv2.threshold(clahe_img, median_bin*2, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Save binary output\n",
    "    preprocessed_images[path.name] = binary\n",
    "    output_path = output_dir / f\"{path.stem}_binary.tiff\"  # Save with _binary suffix\n",
    "    cv2.imwrite(str(output_path), binary)\n",
    "\n",
    "\n",
    "# Display one result for verification\n",
    "sample_key = list(preprocessed_images.keys())[0]\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.imshow(preprocessed_images[sample_key], cmap='gray')\n",
    "plt.title(f\"Preprocessed Binary Image: {sample_key}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a5423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.measure import label, regionprops\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "area_threshold = 75  # minimum area to consider\n",
    "eccentricity_threshold = 0.9  # classify based on this\n",
    "min_dist = 10  # pixels to consider two centroids \"too close\"\n",
    "\n",
    "# Setup paths\n",
    "binary_dir = preprocessed_images  # dictionary: {filename: binary_image}\n",
    "originals = images_8bit           # dictionary: {filename: 8bit_original}\n",
    "stars_dir = Path(\"stars-images\")\n",
    "streaks_dir = Path(\"streaks-images\")\n",
    "stars_dir.mkdir(exist_ok=True)\n",
    "streaks_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Output data\n",
    "all_eccentricity_data = []\n",
    "all_image_stats = []\n",
    "\n",
    "for image_name, binary in binary_dir.items():\n",
    "    original = preprocessed_images[image_name]\n",
    "\n",
    "    # Label connected components\n",
    "    labels = label(binary)\n",
    "    regions = regionprops(labels)\n",
    "\n",
    "    star_objects = []\n",
    "    streak_objects = []\n",
    "    filtered_regions = []\n",
    "\n",
    "    for region in regions:\n",
    "        if region.area < area_threshold:\n",
    "            continue\n",
    "\n",
    "        centroid = region.centroid\n",
    "        if any(np.linalg.norm(np.array(centroid) - np.array(r.centroid)) < min_dist for r in filtered_regions):\n",
    "            continue  # Filter duplicates\n",
    "\n",
    "        filtered_regions.append(region)\n",
    "        eccentricity = region.eccentricity\n",
    "        bbox = region.bbox\n",
    "        x, y = int(centroid[0]), int(centroid[1])\n",
    "\n",
    "        # Crop the region from original image\n",
    "        crop = original[bbox[0]:bbox[2], bbox[1]:bbox[3]]\n",
    "\n",
    "        if eccentricity < eccentricity_threshold:\n",
    "            obj_type = \"star\"\n",
    "            star_objects.append(region)\n",
    "            save_path = stars_dir / f\"star_{image_name[:-5]}_{x}_{y}.png\"\n",
    "        else:\n",
    "            obj_type = \"streak\"\n",
    "            streak_objects.append(region)\n",
    "            save_path = streaks_dir / f\"streak_{image_name[:-5]}_{x}_{y}.png\"\n",
    "\n",
    "        # Save cropped image\n",
    "        cv2.imwrite(str(save_path), crop)\n",
    "\n",
    "        # Log object data\n",
    "        all_eccentricity_data.append({\n",
    "            'image': image_name,\n",
    "            'object_type': obj_type,\n",
    "            'object_name': os.path.basename(save_path),\n",
    "            'eccentricity': eccentricity\n",
    "        })\n",
    "\n",
    "    # Log image-wise summary\n",
    "    all_image_stats.append({\n",
    "        'image_name': image_name,\n",
    "        'no_of_stars': len(star_objects),\n",
    "        'no_of_streaks': len(streak_objects)\n",
    "    })\n",
    "\n",
    "# Save logs as CSVs\n",
    "pd.DataFrame(all_eccentricity_data).to_csv('eccentricity_data.csv', index=False)\n",
    "pd.DataFrame(all_image_stats).to_csv('image_stats.csv', index=False)\n",
    "\n",
    "print(\"\\n✅ Connected components processed. Cropped images and CSVs saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0daf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7abafb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature extraction complete. Saved to 'component_features.csv'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Directories\n",
    "star_dir = Path(\"stars-images\")\n",
    "streak_dir = Path(\"streaks-images\")\n",
    "\n",
    "# Output list\n",
    "feature_list = []\n",
    "\n",
    "# Function to extract features from a grayscale image\n",
    "def extract_features(image_path, label=None):\n",
    "    img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    # Threshold for contour detection\n",
    "    _, thresh = cv2.threshold(img, 30, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contours:\n",
    "        return None\n",
    "\n",
    "    cnt = max(contours, key=cv2.contourArea)  # use largest contour\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    aspect_ratio = w / h if h != 0 else 0\n",
    "    area = cv2.contourArea(cnt)\n",
    "\n",
    "    # Eccentricity\n",
    "    M = cv2.moments(cnt)\n",
    "    if M['mu20'] + M['mu02'] == 0:\n",
    "        eccentricity = 0\n",
    "    else:\n",
    "        numerator = (M['mu20'] - M['mu02'])**2 + 4*M['mu11']**2\n",
    "        denominator = (M['mu20'] + M['mu02'])**2\n",
    "        eccentricity = np.sqrt(numerator / denominator) if denominator != 0 else 0\n",
    "\n",
    "    # Hu Moments (log scaled)\n",
    "    hu = cv2.HuMoments(M).flatten()\n",
    "    hu = -np.sign(hu) * np.log10(np.abs(hu) + 1e-10)\n",
    "\n",
    "    # Combine features\n",
    "    features = {\n",
    "        'file': image_path.name,\n",
    "        'label': label,\n",
    "        'aspect_ratio': aspect_ratio,\n",
    "        'area': area,\n",
    "        'eccentricity': eccentricity\n",
    "    }\n",
    "\n",
    "    for i, h in enumerate(hu):\n",
    "        features[f'hu{i+1}'] = h\n",
    "\n",
    "    return features\n",
    "\n",
    "# Extract from star images\n",
    "for path in star_dir.glob(\"*.png\"):\n",
    "    feat = extract_features(path, label=\"star\")\n",
    "    if feat: feature_list.append(feat)\n",
    "\n",
    "# Extract from streak images\n",
    "for path in streak_dir.glob(\"*.png\"):\n",
    "    feat = extract_features(path, label=\"streak\")\n",
    "    if feat: feature_list.append(feat)\n",
    "\n",
    "# Convert to DataFrame\n",
    "features_df = pd.DataFrame(feature_list)\n",
    "features_df.to_csv(\"component_features.csv\", index=False)\n",
    "\n",
    "print(\"✅ Feature extraction complete. Saved to 'component_features.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16570bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Processing complete. Annotated images, cropped objects, and CSVs saved.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.measure import label, regionprops\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "area_threshold = 75  # minimum area to consider\n",
    "eccentricity_threshold = 0.9  # classify based on this\n",
    "min_dist = 10  # pixels to consider two centroids \"too close\"\n",
    "\n",
    "# Setup paths\n",
    "binary_dir = preprocessed_images  # dictionary: {filename: binary_image}\n",
    "originals = images_8bit           # dictionary: {filename: 8bit_original}\n",
    "output_dir = Path(\"annotated_images\")\n",
    "stars_dir = Path(\"stars-images\")\n",
    "streaks_dir = Path(\"streaks-images\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "stars_dir.mkdir(exist_ok=True)\n",
    "streaks_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Output data\n",
    "all_eccentricity_data = []\n",
    "all_image_stats = []\n",
    "\n",
    "for image_name, binary in binary_dir.items():\n",
    "    original = originals[image_name]\n",
    "    \n",
    "    # Convert grayscale to BGR for color annotations if needed\n",
    "    if len(original.shape) == 2:\n",
    "        annotated_img = cv2.cvtColor(original, cv2.COLOR_GRAY2BGR)\n",
    "    else:\n",
    "        annotated_img = original.copy()\n",
    "    \n",
    "    # Label connected components\n",
    "    labels = label(binary)\n",
    "    regions = regionprops(labels)\n",
    "\n",
    "    star_objects = []\n",
    "    streak_objects = []\n",
    "    filtered_regions = []\n",
    "\n",
    "    for region in regions:\n",
    "        if region.area < area_threshold:\n",
    "            continue\n",
    "\n",
    "        centroid = region.centroid\n",
    "        if any(np.linalg.norm(np.array(centroid) - np.array(r.centroid)) < min_dist for r in filtered_regions):\n",
    "            continue  # Filter duplicates\n",
    "\n",
    "        filtered_regions.append(region)\n",
    "        eccentricity = region.eccentricity\n",
    "        bbox = region.bbox\n",
    "        y1, x1, y2, x2 = bbox  # Note: regionprops uses (min_row, min_col, max_row, max_col)\n",
    "        \n",
    "        # Convert to (x,y,w,h) format for OpenCV\n",
    "        x, y, w, h = x1, y1, x2 - x1, y2 - y1\n",
    "        \n",
    "        # Crop the region from original image\n",
    "        crop = original[y1:y2, x1:x2]\n",
    "\n",
    "        if eccentricity < eccentricity_threshold:\n",
    "            obj_type = \"star\"\n",
    "            color = (0, 255, 0)  # Green for stars\n",
    "            star_objects.append(region)\n",
    "            save_path = stars_dir / f\"star_{image_name[:-5]}_{x}_{y}.png\"\n",
    "        else:\n",
    "            obj_type = \"streak\"\n",
    "            color = (0, 0, 255)  # Red for streaks\n",
    "            streak_objects.append(region)\n",
    "            save_path = streaks_dir / f\"streak_{image_name[:-5]}_{x}_{y}.png\"\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(annotated_img, (x1, y1), (x2, y2), color, 2)\n",
    "        \n",
    "        # Add label text\n",
    "        label_text = f\"{obj_type} {eccentricity:.2f}\"\n",
    "        cv2.putText(annotated_img, label_text, (x1, y1 - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "        \n",
    "        # Save cropped image\n",
    "        cv2.imwrite(str(save_path), crop)\n",
    "\n",
    "        # Log object data\n",
    "        all_eccentricity_data.append({\n",
    "            'image': image_name,\n",
    "            'object_type': obj_type,\n",
    "            'object_name': os.path.basename(save_path),\n",
    "            'eccentricity': eccentricity,\n",
    "            'bbox_x':    x1,\n",
    "            'bbox_y': y1,\n",
    "            'bbox_width': w,\n",
    "            'bbox_height': h\n",
    "        })\n",
    "\n",
    "    # Save annotated image\n",
    "    annotated_path = output_dir / f\"annotated_{image_name}\"\n",
    "    cv2.imwrite(str(annotated_path), annotated_img)\n",
    "    \n",
    "    # Log image-wise summary\n",
    "    all_image_stats.append({\n",
    "        'image_name': image_name,\n",
    "        'no_of_stars': len(star_objects),\n",
    "        'no_of_streaks': len(streak_objects)\n",
    "    })\n",
    "\n",
    "# Save logs as CSVs\n",
    "pd.DataFrame(all_eccentricity_data).to_csv('eccentricity_data.csv', index=False)\n",
    "pd.DataFrame(all_image_stats).to_csv('image_stats.csv', index=False)\n",
    "\n",
    "print(\"\\n✅ Processing complete. Annotated images, cropped objects, and CSVs saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0fe811c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load extracted features\n",
    "df = pd.read_csv(\"component_features.csv\")\n",
    "\n",
    "# Preserve true labels for comparison\n",
    "true_labels = df['label']\n",
    "\n",
    "# Select only numerical features (ignore filename + label)\n",
    "X = df.drop(columns=['file', 'label'])\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA to 2 components for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "df['pca1'] = X_pca[:, 0]\n",
    "df['pca2'] = X_pca[:, 1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, true_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# KMeans clustering (2 clusters)\n",
    "kmeans = KMeans(n_clusters=2, random_state=4)\n",
    "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='pca1', y='pca2', hue='cluster', style=true_labels, palette='Set2', s=60)\n",
    "plt.title(\"PCA Projection of Cropped Objects with KMeans Clusters\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend(title='Cluster/True Label')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f275f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Cluster-to-Label Mapping (Majority Vote): {np.int32(0): 'star', np.int32(1): 'streak'}\n",
      "\n",
      "🧾 Confusion Matrix:\n",
      "               Predicted Star  Predicted Streak\n",
      "Actual Star               383                 0\n",
      "Actual Streak               0                10\n",
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        star       1.00      1.00      1.00       383\n",
      "      streak       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00       393\n",
      "   macro avg       1.00      1.00      1.00       393\n",
      "weighted avg       1.00      1.00      1.00       393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Majority voting to map cluster ID → actual label (star/streak)\n",
    "cluster_to_label = {}\n",
    "\n",
    "for cluster_id in df['cluster'].unique():\n",
    "    actual_labels = df[df['cluster'] == cluster_id]['label']\n",
    "    most_common = actual_labels.value_counts().idxmax()\n",
    "    cluster_to_label[cluster_id] = most_common\n",
    "\n",
    "print(\"🔁 Cluster-to-Label Mapping (Majority Vote):\", cluster_to_label)\n",
    "\n",
    "# Map clusters to predicted labels\n",
    "df['predicted_label'] = df['cluster'].map(cluster_to_label)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(df['label'], df['predicted_label'], labels=['star', 'streak'])\n",
    "report = classification_report(df['label'], df['predicted_label'], target_names=['star', 'streak'])\n",
    "\n",
    "print(\"\\n🧾 Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, index=['Actual Star', 'Actual Streak'], columns=['Predicted Star', 'Predicted Streak']))\n",
    "\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "970dea39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clustered predictions saved to 'clustered_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Select relevant columns for export\n",
    "output_df = df[['file', 'label', 'predicted_label', 'cluster', 'pca1', 'pca2']]\n",
    "\n",
    "# Save to CSV\n",
    "output_df.to_csv(\"clustered_predictions.csv\", index=False)\n",
    "\n",
    "print(\"✅ Clustered predictions saved to 'clustered_predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e21ad09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31993d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641565f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e993470e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ec61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1386ada6",
   "metadata": {},
   "source": [
    "**USED THRESHOLD PATCHES TO TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc8b6244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classes: ['stars', 'streaks']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.0474 - Val Acc: 0.9812\n",
      "Epoch 2/5 - Loss: 0.0005 - Val Acc: 0.9906\n",
      "Epoch 3/5 - Loss: 0.0002 - Val Acc: 0.9906\n",
      "Epoch 4/5 - Loss: 0.0001 - Val Acc: 0.9906\n",
      "Epoch 5/5 - Loss: 0.0001 - Val Acc: 1.0000\n",
      "\n",
      "📊 Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       stars       1.00      1.00      1.00       209\n",
      "     streaks       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00       213\n",
      "   macro avg       1.00      1.00      1.00       213\n",
      "weighted avg       1.00      1.00      1.00       213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "random_seed = 4 \n",
    "random.seed(random_seed)\n",
    " \n",
    "np.random.seed(random_seed)\n",
    " \n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)  \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False \n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# --- DATASET ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # 1 channel input\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# ImageFolder assumes each class is in a folder\n",
    "dataset = datasets.ImageFolder(root=r'Data\\Dataset_thresh', transform=transform)\n",
    "\n",
    "# Print class mapping\n",
    "print(\"✅ Classes:\", dataset.classes)  # ['stars-images', 'streaks-images']\n",
    "\n",
    "# --- SPLIT ---\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "train_targets = [dataset.targets[i] for i in train_ds.indices]\n",
    "class_sample_count = Counter(train_targets)\n",
    "\n",
    "# Inverse frequency for sampling\n",
    "weights = [1.0 / class_sample_count[t] for t in train_targets]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "# DataLoader with sampler\n",
    "train_loader = DataLoader(train_ds, batch_size=32, sampler=sampler)\n",
    "val_loader = DataLoader(val_ds, batch_size=32)\n",
    "\n",
    "    \n",
    "# --- MODEL ---\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 2)\n",
    "model = resnet18.to(device)\n",
    "\n",
    "# --- TRAINING SETUP ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 5\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# --- TRAIN LOOP ---\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # --- VALIDATION ---\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_preds, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_train_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_starstreak_model.pth\")\n",
    "\n",
    "# --- EVALUATION ---\n",
    "print(\"\\n📊 Final Classification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc6ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11bfdc98",
   "metadata": {},
   "source": [
    "**RAW DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db4b0b9",
   "metadata": {},
   "source": [
    "*without augment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5606c530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classes: ['stars', 'streaks']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.0308 - Val Acc: 0.9973\n",
      "Epoch 2/5 - Loss: 0.0070 - Val Acc: 1.0000\n",
      "Epoch 3/5 - Loss: 0.0004 - Val Acc: 1.0000\n",
      "Epoch 4/5 - Loss: 0.0001 - Val Acc: 1.0000\n",
      "Epoch 5/5 - Loss: 0.0072 - Val Acc: 1.0000\n",
      "\n",
      "📊 Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       stars       1.00      1.00      1.00       364\n",
      "     streaks       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00       370\n",
      "   macro avg       1.00      1.00      1.00       370\n",
      "weighted avg       1.00      1.00      1.00       370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "random_seed = 4 \n",
    "random.seed(random_seed)\n",
    " \n",
    "np.random.seed(random_seed)\n",
    " \n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)  \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False \n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# --- DATASET ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # 1 channel input\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# ImageFolder assumes each class is in a folder\n",
    "dataset = datasets.ImageFolder(root=r'Data\\Dataset_Raw', transform=transform)\n",
    "\n",
    "# Print class mapping\n",
    "print(\"✅ Classes:\", dataset.classes)  # ['stars-images', 'streaks-images']\n",
    "\n",
    "# --- SPLIT ---\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "train_targets = [dataset.targets[i] for i in train_ds.indices]\n",
    "class_sample_count = Counter(train_targets)\n",
    "\n",
    "# Inverse frequency for sampling\n",
    "weights = [1.0 / class_sample_count[t] for t in train_targets]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "# DataLoader with sampler\n",
    "train_loader = DataLoader(train_ds, batch_size=32, sampler=sampler)\n",
    "val_loader = DataLoader(val_ds, batch_size=32)\n",
    "\n",
    "    \n",
    "# --- MODEL ---\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 2)\n",
    "model = resnet18.to(device)\n",
    "\n",
    "# --- TRAINING SETUP ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 5\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# --- TRAIN LOOP ---\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # --- VALIDATION ---\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_preds, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_train_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_starstreak_model.pth\")\n",
    "\n",
    "# --- EVALUATION ---\n",
    "print(\"\\n📊 Final Classification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "079c3156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classes: ['stars', 'streaks']\n",
      "✅ Total samples: 2225\n",
      "✅ Class sample counts: Counter({0: 1461, 1: 319})\n",
      "\n",
      "Dataset sizes - Train: 1780, Val: 445\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- MODEL CLASS ---\n",
    "class StarStreakClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StarStreakClassifier, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- DATASET ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load dataset with error handling\n",
    "try:\n",
    "    dataset = datasets.ImageFolder(root='Data/Dataset_Raw_Augment', transform=transform)\n",
    "    print(\"✅ Classes:\", dataset.classes)\n",
    "    print(f\"✅ Total samples: {len(dataset)}\")\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"Dataset is empty! Check your 'Data' directory structure.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- SPLIT ---\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Add check for minimum dataset size\n",
    "if train_size == 0 or val_size == 0:\n",
    "    raise ValueError(\"Not enough samples for training and validation split\")\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# --- SAMPLER ---\n",
    "# Get labels from the original dataset (not the split)\n",
    "\n",
    "train_targets = [dataset.targets[i] for i in train_ds.indices]\n",
    "class_sample_count = Counter(train_targets)\n",
    "\n",
    "print(f\"✅ Class sample counts: {class_sample_count}\")\n",
    "# Inverse frequency for sampling\n",
    "weights = [1.0 / class_sample_count[t] for t in train_targets]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "# DataLoader with sampler\n",
    "train_loader = DataLoader(train_ds, batch_size=32, sampler=sampler)\n",
    "val_loader = DataLoader(val_ds, batch_size=32)\n",
    "\n",
    "print(f\"\\nDataset sizes - Train: {len(train_ds)}, Val: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6866c9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.0885 - Val Acc: 0.9820\n",
      "Epoch 2/5 - Loss: 0.0069 - Val Acc: 1.0000\n",
      "Epoch 3/5 - Loss: 0.0072 - Val Acc: 1.0000\n",
      "Epoch 4/5 - Loss: 0.0011 - Val Acc: 1.0000\n",
      "Epoch 5/5 - Loss: 0.0012 - Val Acc: 1.0000\n",
      "\n",
      "📊 Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       stars       1.00      1.00      1.00       359\n",
      "     streaks       1.00      1.00      1.00        86\n",
      "\n",
      "    accuracy                           1.00       445\n",
      "   macro avg       1.00      1.00      1.00       445\n",
      "weighted avg       1.00      1.00      1.00       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- MODEL INITIALIZATION ---\n",
    "model = StarStreakClassifier().to(device)\n",
    "\n",
    "# --- TRAINING SETUP ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 5\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# --- TRAIN LOOP ---\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # --- VALIDATION ---\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_preds, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_train_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_starstreak_model.pth\")\n",
    "\n",
    "# --- EVALUATION ---\n",
    "print(\"\\n📊 Final Classification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff558d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eec3d382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train class distribution: Counter({0: 1461, 1: 319})\n",
      "Balanced train class distribution: Counter({0: 319, 1: 319})\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "# --- MANUAL UNDERSAMPLING ---\n",
    "# Extract targets from training set\n",
    "targets = [dataset.targets[i] for i in train_ds.indices]\n",
    "class_sample_count = Counter(targets)\n",
    "print(\"Original train class distribution:\", class_sample_count)\n",
    "\n",
    "# Find the minority class count\n",
    "min_class_count = min(class_sample_count.values())\n",
    "\n",
    "# Build balanced indices\n",
    "balanced_indices = []\n",
    "class_indices = {cls: [] for cls in class_sample_count}\n",
    "\n",
    "for idx in train_ds.indices:\n",
    "    label = dataset.targets[idx]\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# Undersample each class\n",
    "for cls, indices in class_indices.items():\n",
    "    undersampled = np.random.choice(indices, min_class_count, replace=False)\n",
    "    balanced_indices.extend(undersampled)\n",
    "\n",
    "# Create new subset with balanced indices\n",
    "train_ds = Subset(dataset, balanced_indices)\n",
    "print(\"Balanced train class distribution:\",\n",
    "      Counter([dataset.targets[i] for i in train_ds.indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13230c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classes: ['stars', 'streaks']\n",
      "✅ Total samples: 2225\n",
      "\n",
      "Dataset sizes - Train: 552, Val: 333, Test: 335\n",
      "\n",
      "Epoch 1/30\n",
      "Train Loss: 0.1973 | Val Loss: 0.7370\n",
      "Train Acc: 0.9221 | Val Acc: 0.6667\n",
      "Learning Rate: 1.00e-04\n",
      "\n",
      "Epoch 2/30\n",
      "Train Loss: 0.0114 | Val Loss: 0.7273\n",
      "Train Acc: 0.9982 | Val Acc: 0.6126\n",
      "Learning Rate: 1.00e-04\n",
      "\n",
      "Epoch 3/30\n",
      "Train Loss: 0.0428 | Val Loss: 0.0056\n",
      "Train Acc: 0.9891 | Val Acc: 1.0000\n",
      "Learning Rate: 1.00e-04\n",
      "\n",
      "Epoch 4/30\n",
      "Train Loss: 0.0148 | Val Loss: 0.0028\n",
      "Train Acc: 0.9928 | Val Acc: 1.0000\n",
      "Learning Rate: 1.00e-04\n",
      "\n",
      "Epoch 5/30\n",
      "Train Loss: 0.0033 | Val Loss: 0.0022\n",
      "Train Acc: 1.0000 | Val Acc: 1.0000\n",
      "Learning Rate: 1.00e-04\n",
      "\n",
      "Epoch 6/30\n",
      "Train Loss: 0.0021 | Val Loss: 0.0016\n",
      "Train Acc: 1.0000 | Val Acc: 1.0000\n",
      "Learning Rate: 1.00e-04\n",
      "\n",
      "Epoch 7/30\n",
      "Train Loss: 0.0015 | Val Loss: 0.0011\n",
      "Train Acc: 1.0000 | Val Acc: 1.0000\n",
      "Learning Rate: 1.00e-04\n",
      "\n",
      "Epoch 8/30\n",
      "Train Loss: 0.0054 | Val Loss: 0.0047\n",
      "Train Acc: 0.9982 | Val Acc: 1.0000\n",
      "Learning Rate: 1.00e-04\n",
      "\n",
      "Epoch 9/30\n",
      "Train Loss: 0.0226 | Val Loss: 0.0008\n",
      "Train Acc: 0.9891 | Val Acc: 1.0000\n",
      "Learning Rate: 1.00e-04\n",
      "\n",
      "Epoch 10/30\n",
      "Train Loss: 0.0023 | Val Loss: 0.0011\n",
      "Train Acc: 1.0000 | Val Acc: 1.0000\n",
      "Learning Rate: 1.00e-04\n",
      "\n",
      "Epoch 11/30\n",
      "Train Loss: 0.0012 | Val Loss: 0.0009\n",
      "Train Acc: 1.0000 | Val Acc: 1.0000\n",
      "Learning Rate: 1.00e-04\n",
      "\n",
      "Epoch 12/30\n",
      "Train Loss: 0.0010 | Val Loss: 0.0008\n",
      "Train Acc: 1.0000 | Val Acc: 1.0000\n",
      "Learning Rate: 1.00e-04\n",
      "\n",
      "Epoch 13/30\n",
      "Train Loss: 0.0008 | Val Loss: 0.0006\n",
      "Train Acc: 1.0000 | Val Acc: 1.0000\n",
      "Learning Rate: 1.00e-04\n",
      "⏹ Early stopping triggered after 13 epochs\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 234\u001b[39m\n\u001b[32m    231\u001b[39m plt.close()\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# --- FINAL EVALUATION ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_model.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    235\u001b[39m model.eval()\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# Test set evaluation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'best_model.pth'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Force CPU usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available = lambda: False  # Override CUDA check\n",
    "\n",
    "# --- MODEL CLASS ---\n",
    "class StarStreakClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StarStreakClassifier, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- CALLBACKS ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > (self.best_loss - self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "class LRScheduler:\n",
    "    def __init__(self, optimizer, patience=3, factor=0.1, min_lr=1e-6):\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self._reduce_lr()\n",
    "                self.counter = 0\n",
    "\n",
    "    def _reduce_lr(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            old_lr = param_group['lr']\n",
    "            new_lr = max(old_lr * self.factor, self.min_lr)\n",
    "            param_group['lr'] = new_lr\n",
    "            print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
    "\n",
    "# --- DATASET ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    dataset = datasets.ImageFolder(root='Data/Dataset_Raw_Augment', transform=transform)\n",
    "    print(\"✅ Classes:\", dataset.classes)\n",
    "    print(f\"✅ Total samples: {len(dataset)}\")\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"Dataset is empty!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- SPLIT ---\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# --- BALANCE TRAINING SET ---\n",
    "targets = [dataset.targets[i] for i in train_ds.indices]\n",
    "class_sample_count = Counter(targets)\n",
    "min_class_count = min(class_sample_count.values())\n",
    "\n",
    "balanced_indices = []\n",
    "class_indices = {cls: [] for cls in class_sample_count}\n",
    "\n",
    "for idx in train_ds.indices:\n",
    "    label = dataset.targets[idx]\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "for cls, indices in class_indices.items():\n",
    "    balanced_indices.extend(np.random.choice(indices, min_class_count, replace=False))\n",
    "\n",
    "train_ds = Subset(dataset, balanced_indices)\n",
    "\n",
    "# --- DATALOADERS ---\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32)\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "print(f\"\\nDataset sizes - Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
    "\n",
    "# --- TRAINING SETUP ---\n",
    "model = StarStreakClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "early_stopping = EarlyStopping(patience=7, min_delta=0.001)\n",
    "lr_scheduler = LRScheduler(optimizer)\n",
    "\n",
    "# --- TRAINING METRICS TRACKING ---\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_val_acc = 0.0\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- TRAIN PHASE ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_train_acc = correct_train / total_train\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_acc)\n",
    "    \n",
    "    # --- VALIDATION PHASE ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "    epoch_val_acc = correct_val / total_val\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_accuracies.append(epoch_val_acc)\n",
    "    \n",
    "    # --- CALLBACKS ---\n",
    "    early_stopping(epoch_val_loss)\n",
    "    lr_scheduler(epoch_val_loss)\n",
    "    \n",
    "    # --- PRINT PROGRESS ---\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "    print(f\"Train Acc: {epoch_train_acc:.4f} | Val Acc: {epoch_val_acc:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if epoch_val_acc > best_val_acc:\n",
    "        best_val_acc = epoch_val_acc\n",
    "        torch.save(model.state_dict(), 'best_model_augment_noiseless.pth')\n",
    "        \n",
    "    # Early stopping check\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"⏹ Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "# --- PLOT TRAINING CURVES ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('training_metrics.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a7085edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Final Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       stars       1.00      1.00      1.00       262\n",
      "     streaks       1.00      1.00      1.00        71\n",
      "\n",
      "    accuracy                           1.00       333\n",
      "   macro avg       1.00      1.00      1.00       333\n",
      "weighted avg       1.00      1.00      1.00       333\n",
      "\n",
      "\n",
      "📊 Test Set Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       stars       1.00      1.00      1.00       277\n",
      "     streaks       1.00      1.00      1.00        58\n",
      "\n",
      "    accuracy                           1.00       335\n",
      "   macro avg       1.00      1.00      1.00       335\n",
      "weighted avg       1.00      1.00      1.00       335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- FINAL EVALUATION ---\n",
    "model.load_state_dict(torch.load('best_model_augment_noiseless.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Test set evaluation\n",
    "test_preds, test_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\n📊 Final Validation Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=dataset.classes))\n",
    "\n",
    "print(\"\\n📊 Test Set Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=dataset.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d591b",
   "metadata": {},
   "source": [
    "**ADDING NOISE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1e9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classes: ['background', 'noise', 'stars', 'streaks']\n",
      "✅ Total samples: 3325\n",
      "Balanced class counts: Counter({1: 288, 2: 288, 0: 288, 3: 288})\n",
      "\n",
      "Dataset sizes - Train: 1152, Val: 498, Test: 500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 154\u001b[39m\n\u001b[32m    151\u001b[39m inputs, labels = inputs.to(device), labels.to(device)\n\u001b[32m    153\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m    156\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mStarStreakClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\resnet.py:275\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    273\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer1(x)\n\u001b[32m    274\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer2(x)\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer4(x)\n\u001b[32m    278\u001b[39m x = \u001b[38;5;28mself\u001b[39m.avgpool(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torchvision\\models\\resnet.py:93\u001b[39m, in \u001b[36mBasicBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     90\u001b[39m identity = x\n\u001b[32m     92\u001b[39m out = \u001b[38;5;28mself\u001b[39m.conv1(x)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu(out)\n\u001b[32m     96\u001b[39m out = \u001b[38;5;28mself\u001b[39m.conv2(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\functional.py:2822\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2820\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Force CPU usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available = lambda: False  # Override CUDA check\n",
    "\n",
    "# --- MODEL CLASS ---\n",
    "class StarStreakClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StarStreakClassifier, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- CALLBACKS ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > (self.best_loss - self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "class LRScheduler:\n",
    "    def __init__(self, optimizer, patience=3, factor=0.1, min_lr=1e-6):\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self._reduce_lr()\n",
    "                self.counter = 0\n",
    "\n",
    "    def _reduce_lr(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            old_lr = param_group['lr']\n",
    "            new_lr = max(old_lr * self.factor, self.min_lr)\n",
    "            param_group['lr'] = new_lr\n",
    "            print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
    "\n",
    "# --- DATASET ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    dataset = datasets.ImageFolder(root='Data/Dataset_Raw_Augment_Noise', transform=transform)\n",
    "    print(\"✅ Classes:\", dataset.classes)\n",
    "    print(f\"✅ Total samples: {len(dataset)}\")\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"Dataset is empty!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- SPLIT ---\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# --- BALANCE TRAINING SET ---\n",
    "targets = [dataset.targets[i] for i in train_ds.indices]\n",
    "\n",
    "class_sample_count = Counter(targets)\n",
    "min_class_count = min(class_sample_count.values())\n",
    "\n",
    "print(\"Original class counts:\", class_sample_count)\n",
    "balanced_indices = []\n",
    "class_indices = {cls: [] for cls in class_sample_count}\n",
    "\n",
    "for idx in train_ds.indices:\n",
    "    label = dataset.targets[idx]\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "for cls, indices in class_indices.items():\n",
    "    balanced_indices.extend(np.random.choice(indices, min_class_count, replace=False))\n",
    "balanced_targets = [dataset.targets[i] for i in balanced_indices]\n",
    "balanced_class_count = Counter(balanced_targets)\n",
    "print(\"Balanced class counts:\", balanced_class_count)\n",
    "train_ds = Subset(dataset, balanced_indices)\n",
    "\n",
    "# --- DATALOADERS ---\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32)\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "print(f\"\\nDataset sizes - Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
    "\n",
    "# --- TRAINING SETUP ---\n",
    "model = StarStreakClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "early_stopping = EarlyStopping(patience=7, min_delta=0.001)\n",
    "lr_scheduler = LRScheduler(optimizer)\n",
    "\n",
    "# --- TRAINING METRICS TRACKING ---\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_val_acc = 0.0\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- TRAIN PHASE ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_train_acc = correct_train / total_train\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_acc)\n",
    "    \n",
    "    # --- VALIDATION PHASE ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "    epoch_val_acc = correct_val / total_val\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_accuracies.append(epoch_val_acc)\n",
    "    \n",
    "    # --- CALLBACKS ---\n",
    "    early_stopping(epoch_val_loss)\n",
    "    lr_scheduler(epoch_val_loss)\n",
    "    \n",
    "    # --- PRINT PROGRESS ---\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "    print(f\"Train Acc: {epoch_train_acc:.4f} | Val Acc: {epoch_val_acc:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if epoch_val_acc > best_val_acc:\n",
    "        best_val_acc = epoch_val_acc\n",
    "        torch.save(model.state_dict(), 'best_model_augment_noise.pth')\n",
    "        \n",
    "    # Early stopping check\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"⏹ Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "# --- PLOT TRAINING CURVES ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('training_metrics.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba55dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- FINAL EVALUATION ---\n",
    "model.load_state_dict(torch.load('best_model_augment_noise.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Test set evaluation\n",
    "test_preds, test_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\n📊 Final Validation Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=dataset.classes))\n",
    "\n",
    "print(\"\\n📊 Test Set Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e9ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ef298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee83cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a748a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for StarStreakClassifier:\n\tMissing key(s) in state_dict: \"model.conv1.weight\", \"model.bn1.weight\", \"model.bn1.bias\", \"model.bn1.running_mean\", \"model.bn1.running_var\", \"model.layer1.0.conv1.weight\", \"model.layer1.0.bn1.weight\", \"model.layer1.0.bn1.bias\", \"model.layer1.0.bn1.running_mean\", \"model.layer1.0.bn1.running_var\", \"model.layer1.0.conv2.weight\", \"model.layer1.0.bn2.weight\", \"model.layer1.0.bn2.bias\", \"model.layer1.0.bn2.running_mean\", \"model.layer1.0.bn2.running_var\", \"model.layer1.1.conv1.weight\", \"model.layer1.1.bn1.weight\", \"model.layer1.1.bn1.bias\", \"model.layer1.1.bn1.running_mean\", \"model.layer1.1.bn1.running_var\", \"model.layer1.1.conv2.weight\", \"model.layer1.1.bn2.weight\", \"model.layer1.1.bn2.bias\", \"model.layer1.1.bn2.running_mean\", \"model.layer1.1.bn2.running_var\", \"model.layer2.0.conv1.weight\", \"model.layer2.0.bn1.weight\", \"model.layer2.0.bn1.bias\", \"model.layer2.0.bn1.running_mean\", \"model.layer2.0.bn1.running_var\", \"model.layer2.0.conv2.weight\", \"model.layer2.0.bn2.weight\", \"model.layer2.0.bn2.bias\", \"model.layer2.0.bn2.running_mean\", \"model.layer2.0.bn2.running_var\", \"model.layer2.0.downsample.0.weight\", \"model.layer2.0.downsample.1.weight\", \"model.layer2.0.downsample.1.bias\", \"model.layer2.0.downsample.1.running_mean\", \"model.layer2.0.downsample.1.running_var\", \"model.layer2.1.conv1.weight\", \"model.layer2.1.bn1.weight\", \"model.layer2.1.bn1.bias\", \"model.layer2.1.bn1.running_mean\", \"model.layer2.1.bn1.running_var\", \"model.layer2.1.conv2.weight\", \"model.layer2.1.bn2.weight\", \"model.layer2.1.bn2.bias\", \"model.layer2.1.bn2.running_mean\", \"model.layer2.1.bn2.running_var\", \"model.layer3.0.conv1.weight\", \"model.layer3.0.bn1.weight\", \"model.layer3.0.bn1.bias\", \"model.layer3.0.bn1.running_mean\", \"model.layer3.0.bn1.running_var\", \"model.layer3.0.conv2.weight\", \"model.layer3.0.bn2.weight\", \"model.layer3.0.bn2.bias\", \"model.layer3.0.bn2.running_mean\", \"model.layer3.0.bn2.running_var\", \"model.layer3.0.downsample.0.weight\", \"model.layer3.0.downsample.1.weight\", \"model.layer3.0.downsample.1.bias\", \"model.layer3.0.downsample.1.running_mean\", \"model.layer3.0.downsample.1.running_var\", \"model.layer3.1.conv1.weight\", \"model.layer3.1.bn1.weight\", \"model.layer3.1.bn1.bias\", \"model.layer3.1.bn1.running_mean\", \"model.layer3.1.bn1.running_var\", \"model.layer3.1.conv2.weight\", \"model.layer3.1.bn2.weight\", \"model.layer3.1.bn2.bias\", \"model.layer3.1.bn2.running_mean\", \"model.layer3.1.bn2.running_var\", \"model.layer4.0.conv1.weight\", \"model.layer4.0.bn1.weight\", \"model.layer4.0.bn1.bias\", \"model.layer4.0.bn1.running_mean\", \"model.layer4.0.bn1.running_var\", \"model.layer4.0.conv2.weight\", \"model.layer4.0.bn2.weight\", \"model.layer4.0.bn2.bias\", \"model.layer4.0.bn2.running_mean\", \"model.layer4.0.bn2.running_var\", \"model.layer4.0.downsample.0.weight\", \"model.layer4.0.downsample.1.weight\", \"model.layer4.0.downsample.1.bias\", \"model.layer4.0.downsample.1.running_mean\", \"model.layer4.0.downsample.1.running_var\", \"model.layer4.1.conv1.weight\", \"model.layer4.1.bn1.weight\", \"model.layer4.1.bn1.bias\", \"model.layer4.1.bn1.running_mean\", \"model.layer4.1.bn1.running_var\", \"model.layer4.1.conv2.weight\", \"model.layer4.1.bn2.weight\", \"model.layer4.1.bn2.bias\", \"model.layer4.1.bn2.running_mean\", \"model.layer4.1.bn2.running_var\", \"model.fc.weight\", \"model.fc.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn1.num_batches_tracked\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.bn2.num_batches_tracked\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn1.num_batches_tracked\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.bn2.num_batches_tracked\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn1.num_batches_tracked\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.bn2.num_batches_tracked\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.0.downsample.1.num_batches_tracked\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn1.num_batches_tracked\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.1.bn2.num_batches_tracked\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.bn1.num_batches_tracked\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.bn2.num_batches_tracked\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.0.downsample.1.num_batches_tracked\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.bn1.num_batches_tracked\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.1.bn2.num_batches_tracked\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.bn1.num_batches_tracked\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.bn2.num_batches_tracked\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.0.downsample.1.num_batches_tracked\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.bn1.num_batches_tracked\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.1.bn2.num_batches_tracked\", \"fc.weight\", \"fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(k.startswith(\u001b[33m'\u001b[39m\u001b[33mmodel.\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m state_dict.keys()):\n\u001b[32m     29\u001b[39m     state_dict = {k.replace(\u001b[33m'\u001b[39m\u001b[33mmodel.\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict.items()}\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m model.eval()\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 2. Define image transformations\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\Streak-and-Stars\\stars\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for StarStreakClassifier:\n\tMissing key(s) in state_dict: \"model.conv1.weight\", \"model.bn1.weight\", \"model.bn1.bias\", \"model.bn1.running_mean\", \"model.bn1.running_var\", \"model.layer1.0.conv1.weight\", \"model.layer1.0.bn1.weight\", \"model.layer1.0.bn1.bias\", \"model.layer1.0.bn1.running_mean\", \"model.layer1.0.bn1.running_var\", \"model.layer1.0.conv2.weight\", \"model.layer1.0.bn2.weight\", \"model.layer1.0.bn2.bias\", \"model.layer1.0.bn2.running_mean\", \"model.layer1.0.bn2.running_var\", \"model.layer1.1.conv1.weight\", \"model.layer1.1.bn1.weight\", \"model.layer1.1.bn1.bias\", \"model.layer1.1.bn1.running_mean\", \"model.layer1.1.bn1.running_var\", \"model.layer1.1.conv2.weight\", \"model.layer1.1.bn2.weight\", \"model.layer1.1.bn2.bias\", \"model.layer1.1.bn2.running_mean\", \"model.layer1.1.bn2.running_var\", \"model.layer2.0.conv1.weight\", \"model.layer2.0.bn1.weight\", \"model.layer2.0.bn1.bias\", \"model.layer2.0.bn1.running_mean\", \"model.layer2.0.bn1.running_var\", \"model.layer2.0.conv2.weight\", \"model.layer2.0.bn2.weight\", \"model.layer2.0.bn2.bias\", \"model.layer2.0.bn2.running_mean\", \"model.layer2.0.bn2.running_var\", \"model.layer2.0.downsample.0.weight\", \"model.layer2.0.downsample.1.weight\", \"model.layer2.0.downsample.1.bias\", \"model.layer2.0.downsample.1.running_mean\", \"model.layer2.0.downsample.1.running_var\", \"model.layer2.1.conv1.weight\", \"model.layer2.1.bn1.weight\", \"model.layer2.1.bn1.bias\", \"model.layer2.1.bn1.running_mean\", \"model.layer2.1.bn1.running_var\", \"model.layer2.1.conv2.weight\", \"model.layer2.1.bn2.weight\", \"model.layer2.1.bn2.bias\", \"model.layer2.1.bn2.running_mean\", \"model.layer2.1.bn2.running_var\", \"model.layer3.0.conv1.weight\", \"model.layer3.0.bn1.weight\", \"model.layer3.0.bn1.bias\", \"model.layer3.0.bn1.running_mean\", \"model.layer3.0.bn1.running_var\", \"model.layer3.0.conv2.weight\", \"model.layer3.0.bn2.weight\", \"model.layer3.0.bn2.bias\", \"model.layer3.0.bn2.running_mean\", \"model.layer3.0.bn2.running_var\", \"model.layer3.0.downsample.0.weight\", \"model.layer3.0.downsample.1.weight\", \"model.layer3.0.downsample.1.bias\", \"model.layer3.0.downsample.1.running_mean\", \"model.layer3.0.downsample.1.running_var\", \"model.layer3.1.conv1.weight\", \"model.layer3.1.bn1.weight\", \"model.layer3.1.bn1.bias\", \"model.layer3.1.bn1.running_mean\", \"model.layer3.1.bn1.running_var\", \"model.layer3.1.conv2.weight\", \"model.layer3.1.bn2.weight\", \"model.layer3.1.bn2.bias\", \"model.layer3.1.bn2.running_mean\", \"model.layer3.1.bn2.running_var\", \"model.layer4.0.conv1.weight\", \"model.layer4.0.bn1.weight\", \"model.layer4.0.bn1.bias\", \"model.layer4.0.bn1.running_mean\", \"model.layer4.0.bn1.running_var\", \"model.layer4.0.conv2.weight\", \"model.layer4.0.bn2.weight\", \"model.layer4.0.bn2.bias\", \"model.layer4.0.bn2.running_mean\", \"model.layer4.0.bn2.running_var\", \"model.layer4.0.downsample.0.weight\", \"model.layer4.0.downsample.1.weight\", \"model.layer4.0.downsample.1.bias\", \"model.layer4.0.downsample.1.running_mean\", \"model.layer4.0.downsample.1.running_var\", \"model.layer4.1.conv1.weight\", \"model.layer4.1.bn1.weight\", \"model.layer4.1.bn1.bias\", \"model.layer4.1.bn1.running_mean\", \"model.layer4.1.bn1.running_var\", \"model.layer4.1.conv2.weight\", \"model.layer4.1.bn2.weight\", \"model.layer4.1.bn2.bias\", \"model.layer4.1.bn2.running_mean\", \"model.layer4.1.bn2.running_var\", \"model.fc.weight\", \"model.fc.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn1.num_batches_tracked\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.bn2.num_batches_tracked\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn1.num_batches_tracked\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.bn2.num_batches_tracked\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn1.num_batches_tracked\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.bn2.num_batches_tracked\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.0.downsample.1.num_batches_tracked\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn1.num_batches_tracked\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.1.bn2.num_batches_tracked\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.bn1.num_batches_tracked\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.bn2.num_batches_tracked\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.0.downsample.1.num_batches_tracked\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.bn1.num_batches_tracked\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.1.bn2.num_batches_tracked\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.bn1.num_batches_tracked\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.bn2.num_batches_tracked\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.0.downsample.1.num_batches_tracked\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.bn1.num_batches_tracked\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.1.bn2.num_batches_tracked\", \"fc.weight\", \"fc.bias\". "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage.measure import label, regionprops\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# 1. Load your trained model\n",
    "class StarStreakClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StarStreakClassifier, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = StarStreakClassifier().to(device)\n",
    "\n",
    "# Load trained weights (handle the model. prefix if needed)\n",
    "state_dict = torch.load(\"best_starstreak_model.pth\", map_location=device)\n",
    "if any(k.startswith('model.') for k in state_dict.keys()):\n",
    "    state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "# 2. Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# 3. Detection and classification parameters\n",
    "area_threshold = 75  # Minimum area to consider\n",
    "min_dist = 10  # Minimum distance between objects\n",
    "\n",
    "def classify_patch(patch):\n",
    "    \"\"\"Classify a single image patch\"\"\"\n",
    "    patch_pil = Image.fromarray(patch)\n",
    "    tensor = transform(patch_pil).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted.item()  # 0=star, 1=streak\n",
    "\n",
    "def process_image(image_path, output_dir):\n",
    "    \"\"\"Process a single image\"\"\"\n",
    "    # Load and prepare image\n",
    "    original = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if original is None:\n",
    "        print(f\"Could not read image: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Create binary image for detection (adjust threshold as needed)\n",
    "    _, binary = cv2.threshold(original, 200, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Prepare output image\n",
    "    annotated_img = cv2.cvtColor(original, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # Label connected components\n",
    "    labels = label(binary)\n",
    "    regions = regionprops(labels)\n",
    "    \n",
    "    for region in regions:\n",
    "        if region.area < area_threshold:\n",
    "            continue\n",
    "            \n",
    "        # Extract bounding box\n",
    "        y1, x1, y2, x2 = region.bbox\n",
    "        \n",
    "        # Crop the region (with some padding)\n",
    "        padding = 5\n",
    "        x1 = max(0, x1 - padding)\n",
    "        y1 = max(0, y1 - padding)\n",
    "        x2 = min(original.shape[1], x2 + padding)\n",
    "        y2 = min(original.shape[0], y2 + padding)\n",
    "        patch = original[y1:y2, x1:x2]\n",
    "        \n",
    "        # Classify the patch\n",
    "        try:\n",
    "            prediction = classify_patch(patch)\n",
    "        except Exception as e:\n",
    "            print(f\"Error classifying patch: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Annotate based on prediction\n",
    "        if prediction == 0:  # Star\n",
    "            color = (0, 255, 0)  # Green\n",
    "            label_text = \"Star\"\n",
    "        else:  # Streak\n",
    "            color = (0, 0, 255)  # Red\n",
    "            label_text = \"Streak\"\n",
    "        \n",
    "        # Draw bounding box and label\n",
    "        cv2.rectangle(annotated_img, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(annotated_img, label_text, (x1, y1 - 5),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "    \n",
    "    # Save annotated image\n",
    "    output_path = os.path.join(output_dir, f\"annotated_{os.path.basename(image_path)}\")\n",
    "    cv2.imwrite(output_path, annotated_img)\n",
    "    print(f\"Saved annotated image: {output_path}\")\n",
    "\n",
    "# 4. Process all images in a directory\n",
    "input_dir = \"Test\"\n",
    "output_dir = \".\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for image_file in os.listdir(input_dir):\n",
    "    if image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff')):\n",
    "        image_path = os.path.join(input_dir, image_file)\n",
    "        process_image(image_path, output_dir)\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab38b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping opencv-contrib-python as it is not installed.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
